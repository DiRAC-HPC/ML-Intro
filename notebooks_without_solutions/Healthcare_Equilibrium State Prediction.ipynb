{"cells": [{"cell_type": "markdown", "metadata": {"id": "xzBGTCzEtyeH"}, "source": ["# 0. Equilibrium thermochemistry for gas regression\n", "\n", "In this notebook, we attempt to build a neural network to calculate the equilibrium thermochemistry for gas under various physical conditions ahead of time.\n", "\n", "Regression models attempt to map from some variables ($x$) to some continuous variables ($y$) and are typically used when either no theoretical model $y=f(x)$ is known or $f(x)$ is expensive to compute. \n", "\n", "Since most gas cooling in astrophysical contexts comes from molecules radiating energy away, their abundances essentially set the cooling rate. Since the temperature affects how the gas evolves dynamically, any physics simulation of processes such as star formation requires a method of accurately calculating the chemistry of the gas.\n", "\n", "Unfortunately, accurately calculating the temperature and chemistry (henceforth thermochemistry) of a gas can be computationally expensive. Many approaches such as simplifying chemical networks to only the most important species and reactions or assuming equilibrium are used to decrease the cost of these calculations. However, even these can fail for large simulations where an object is broken up into many parts which all require a thermochemical calculation.\n", "\n", "As the relationship between $x$ and $y$ becomes more complex, more complex models are required. In this notebook, we will consider a complex case where a deep neural network is required to produce an adequate model.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 184, "status": "ok", "timestamp": 1646400612793, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "DoGOXydmtyeL", "outputId": "07e9ecdf-294b-4cba-ac4b-ad0eda7d3597"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Input\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from os.path import join\n", "\n", "#sklearn\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.metrics import mean_squared_error\n", "from sklearn.preprocessing import StandardScaler\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n"]}, {"cell_type": "markdown", "metadata": {"id": "oXyhjtXQvPCr"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Nt5JP2a7vQBA"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 13, "status": "ok", "timestamp": 1646400613216, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "PqGxA2sTvRWR", "outputId": "aaab5cc6-0308-4ee7-fd86-ea735bb007fb"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "fBcFYqkQtyeM"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "8uIw-Vm1tyeN"}, "source": ["# 1. Load the dataset\n", "\n", "We start by loading the dataset taken from [Chemulator - Holdship et al. 2020](https://ui.adsabs.harvard.edu/abs/2021A%26A...653A..76H/abstract).\n", "\n", "The full **Chemulator** dataset contains the abundances and temperature of simulated astrophysical gases as a function of time when exposed to different conditions. The conditions are described by four input variables:\n", "\n", "- Gas density (`Density`)\n", "- Local UV field strength (`local_uv`)\n", "- Local cosmic ray ionization rate (`zeta`)\n", "- Ratio of the abundance of metals in this gas to the sollar abundance of metal (`metallicity`)\n", "\n", "In this work, we use a slimmed down dataset which contains the equilibrium abundances and temperature for each set of conditions. We will attempt to predict the equilibrium gas temperature.\n", "\n", "Equilibrium modelling is common in astrophysics. For example, stars in the process of forming tend to be surrounded by disks of gas and dust and the equilibrium state of such disks are often modelled. This is done by repeatedly solving the physical equilbrium for a disk with a given temperature distribtion and then the thermochemical equilibrium of a disk with fixed physics. By iterating back and forth, we eventually find an overall equilbrium between chemistry and physics. Such an iterative procedure would made much more efficient by making the chemical equilibrium quick to compute.\n", "\n", "The data are stored in the hdf file 'NHS/equilibrium-small-network.hdf'. In addition local_uv can be calculated by radfield$\\cdot$exp(-3.02$\\cdot$av)\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# load dataset\n", "X_train=pd.read_hdf(join(data_path, 'NHS/equilibrium-small-network.hdf'),key=\"df\")\n", "X_train['local_uv']=X_train['radfield']*np.exp(-3.02*X_train['av'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SxT1G_W-tyeO"}, "outputs": [], "source": ["#grab the features, everything else is a possible target for regression\n", "features=['Density', 'local_uv', 'zeta', 'metallicity']\n", "labels=['gasTemp']\n", "\n", "#we assume a minimum value below which a variable is basically zero\n", "#we also know some variables vary over orders of magnitude but all scales are equally important\n", "#so we log those variables.\n", "X_train[features]=np.where(X_train[features]<1e-5,1e-5,X_train[features])\n", "X_train[['Density', 'local_uv', 'zeta']]=np.log10(X_train[['Density', 'local_uv', 'zeta']])\n", "X_train[labels]=np.where(X_train[labels]<1e-20,1e-20,X_train[labels])\n", "X_train[labels]=np.log10(X_train[labels])\n", "\n", "#data normalisation to set mean 0 std 1\n", "scaler = StandardScaler()\n", "scaler.fit(X_train[features])\n", "\n", "X_train,X_test,y_train,y_test=train_test_split(X_train[features],X_train[labels],test_size=0.25)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "F_vlpuqwtyeO"}, "source": ["# 2. Build the network\n", "\n", "We will build a simple linear regression model and a fully-connected neural network to compare their results. The output is continuous values and so we use 'mean squared error loss'. \n", "\n"]}, {"cell_type": "markdown", "metadata": {"id": "khu4djJZx2_g"}, "source": ["### Linear regression model\n", "\n", "We use linear regression model from sklearn\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model=LinearRegression(normalize=True)\n", "model.fit(X_train,y_train)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 10, "status": "ok", "timestamp": 1646400613445, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "DI5a9qLJtyeP", "outputId": "57c39fde-b9ab-4874-9b0f-ff5e76cc0ff7"}, "outputs": [], "source": ["mse=mean_squared_error(y_test,model.predict(X_test))\n", "print('mse on the test set: {:.2f}'.format(mse))"]}, {"cell_type": "markdown", "metadata": {"id": "k_71L-6htyeP"}, "source": ["### Build a Neural Network\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "def create_model(layers,n_targets,activation=\"swish\",loss=\"mse\"):\n", "    # build the network architecture\n", "    model = Sequential()\n", "    model.add(Input(len(features)))\n", "    for nodes in layers:\n", "        model.add(Dense(nodes, activation=activation))\n", "    model.add(Dense(n_targets, activation=activation))\n", "    # optimizer, loss, metrics\n", "    optimizer = keras.optimizers.Adam(lr=0.001)\n", "    model.compile(optimizer=optimizer,loss=loss)\n", "    return model\n", "\n", "layers=[512,512]\n", "model=create_model(layers,n_targets = 1)\n", "\n", "# print summary\n", "model.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 238, "status": "ok", "timestamp": 1646400613680, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "EIBvkg5CtyeQ", "outputId": "e79643d6-c62e-4612-dd7c-d64b2ece9de6"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "gBb9T19JtyeQ"}, "source": ["### Train the model\n", "\n", "We train the model using our training data and after each full pass of the training data (an epoch), we evaluate it on the test data. We repeat this process until the validation loss stops decreasing. Beyond this point, the model may start to overfit to the training data if the dataset is small or poorly sampled. To avoid this, we use Keras' early stopping to end training once the validation loss does not improve for two epochs. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# train the model\n", "\n", "stopper=EarlyStopping( monitor =\"val_loss\", min_delta=0.0,verbose=1,\n", "                      mode=\"min\",\n", "                      restore_best_weights=True,patience=2)\n", "Normalised_X_train = scaler.transform(X_train)\n", "Normalised_X_test = scaler.transform(X_test)\n", "training_history = model.fit(Normalised_X_train, y_train, epochs=99, batch_size=32, \n", "                             validation_data=(Normalised_X_test, y_test),callbacks=[stopper])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 45672, "status": "ok", "timestamp": 1646400659349, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "MaeUZgS2tyeQ", "outputId": "43f87c5e-9605-46ff-b3ca-8d093dd486a7"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "kcBtJ33sZuWe"}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {"id": "eYtY0XoztyeR"}, "source": ["After we've trained the model, we can plot the loss as a function of epoch. For this data, we can see that both losses decrease up to a point and then become stable. This is common when the dataset is large enough to prevent overfitting. At some point, the model learns the best relationship it can between the input and output variables and stops improving. Where the validation loss is much larger than the training loss, it is likely overfitting has occurred.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(training_history.history['loss'], label='Loss on training data')\n", "plt.plot(training_history.history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 385}, "executionInfo": {"elapsed": 501, "status": "ok", "timestamp": 1646400659844, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "2GkILA_jtyeR", "outputId": "ba84c765-fbe3-4fd2-f98c-d804937e5943"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "GQjCaiqBtyeR"}, "source": ["The MSE is a useful metric and is a good summary of the general performance of the model. However, it can be instructive to look more closely at the predictions. In this section, we make a prediction of the temperature for every entry in our test data. We can then plot predicted vs real temperature and check to see if the performance of the model greatly varies depending on the temperature it is required to predict."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "ExWTo9J6tyeR"}, "outputs": [], "source": ["# use test images to predict\n", "Normalised_X_test = scaler.transform(X_test)\n", "y_pred = model.predict(Normalised_X_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "lcXUeizatyeR"}, "outputs": [], "source": ["pred_df=pd.DataFrame({\"x\":y_test.values[:,0].flatten(),\"y\":y_pred[:,0].flatten()})"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 311}, "executionInfo": {"elapsed": 6933, "status": "ok", "timestamp": 1646400667058, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "ElnmWfYftyeS", "outputId": "fef33055-2e61-4ab3-dc4c-15564c7ffb57"}, "outputs": [], "source": ["# get the indices of wrong predictions\n", "fig,ax=plt.subplots(figsize=(8,4.5))\n", "ax=sns.kdeplot(data=pred_df,x=\"x\",y=\"y\",ax=ax,clip=(1.0,4.0),fill=True)\n", "ax.plot(y_test.values[:,0],y_test.values[:,0],color=\"black\",lw=2)\n", "settings=ax.set(xlabel=\"Gas Temperature / K\",ylabel=\"Prediction\",xlim=(1.0,4.0),ylim=(1.0,4.0))"]}, {"cell_type": "markdown", "metadata": {"id": "5cGFTvxXtyeS"}, "source": ["We can further plot the MSE as a function of each parameter to see how the model performs depending on the area of parameter space."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "J2EvPo15tyeS"}, "outputs": [], "source": ["pred_df=pred_df.merge(X_test.round(2).reset_index(),left_index=True,right_index=True)\n", "pred_df[\"error\"]=(pred_df[\"y\"]-pred_df[\"x\"]).pow(2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 551}, "executionInfo": {"elapsed": 766, "status": "ok", "timestamp": 1646400667821, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "K2Z6fKCptyeS", "outputId": "b37d64d2-cafb-4fe4-c8ce-68dcd548fd84"}, "outputs": [], "source": ["fig,axes=plt.subplots(2,2,figsize=(16,9))\n", "axes=axes.flatten()\n", "for i,feature in enumerate(features):\n", "    sns.histplot(data=pred_df,x=feature,y=\"error\",ax=axes[i])"]}, {"cell_type": "markdown", "metadata": {"id": "oNzkXajbtyeS"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "g8bEaKjctyeS"}, "source": ["# 4. Exercises\n", "\n", "### Model Tuning\n", "\n", "In this notebook, we simply produce a neural network and train it with no explanation for why the model hyperparameters were chosen. However, the ``create_model()`` function takes each major hyperparameter of the model as an input and can therefore be used to easily produce many models and test their performance. \n", "\n", "Use a grid search to find optimal parameters for the model. \n", "\n", "- Can you significantly improve the performance?\n", "- Which activation function works best?\n", "- How many layers are optimal?\n", "\n", "### Extension\n", "- Is MSE the best loss to use? How can you compare models trained with different losses?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "DGKYQ-fktyeT"}, "outputs": [], "source": ["layer_configs=[]#a list of lists\n", "activation_functions=[]#a list of strings\n", "performance_df=pd.DataFrame(columns=[\"layers\",\"activation\",\"MSE\"])\n", "\n", "#create a loop over your hyperparametrs\n", "\n", "    #within that loop: create, train and evaluate a model then store its mse\n", "    \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "mg5qVL8IHjEV"}, "outputs": [], "source": []}], "metadata": {"colab": {"collapsed_sections": [], "name": "NHS_Equilibrium state prediction_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}