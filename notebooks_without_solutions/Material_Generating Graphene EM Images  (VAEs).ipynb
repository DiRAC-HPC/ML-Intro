{"cells": [{"cell_type": "markdown", "metadata": {"id": "XBJKoD0jw8ED"}, "source": ["# 0. Generating Graphene EM Images using Variational Autoencoders (VAEs).\n", "\n", "In this notebook, we attempt to build Variational Autoencoders (VAEs) to generate new samples plausibly drawn from the training dataset.\n", "\n", "Inelastic neutron scattering (INS) can be used to infer information about the forces present in a material. Neutrons scatter off a sample, exchanging energy with certain fundamental vibrational modes of the sample. These vibraional modes include phonons (interatomic boding forces) and magnons (spin coupling between magnetic nuclei). \n", "\n", "[Johnstone et al. (2012)](https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.109.237202) have simulated magnon spectra from a double perovskite systems, where INS was used to distinguish between two possible magnetic Hamiltonians of the system. For this practical, we have simulated datasets for each of the possible Hamiltonians. We are going to train a CNN to classify the system correctly.\n", "\n", "\n", "The aim of this work is to make a disentangled variational autoencoder ($\\beta$-VAE) to generate new images from the INS dataset, using CNNs for encoding and decoding. \n", "\n", "Compared to a simple VAE, a $\\beta$-VAE only introduce one hyperparameter $\\beta$ to the loss function to balance the effects of the reconstruction loss and the variational loss. A simple VAE is the special case with $\\beta=1$. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 6658, "status": "ok", "timestamp": 1646396736920, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "u66arUoew8EI", "outputId": "a3af2ee9-064d-4b4b-b7b4-929c4ba68fd0"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import h5py\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)"]}, {"cell_type": "markdown", "metadata": {"id": "JofLYYQ78J7T"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 13694, "status": "ok", "timestamp": 1646396750605, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "0uekVlam8ILu"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 20639, "status": "ok", "timestamp": 1646396773438, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Nd-_WpGE8MBH", "outputId": "a2e5a0a2-f3b3-41b5-dc43-e398815efe70"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "vHBf9A7r8OY4"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "mN9KpPs6w8EL"}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "We have already split the data into training and validation sets and saved them into two HDF5 files, `ins-data/train.h5`, containing 20,000 INS images.\n", "\n", "### The `tf.data.Dataset` class\n", "The number of images is so large that we may not be able to simultaneously load the whole dataset into memory on a small machine. To solve this issue, we will use [tensorflow.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to create an interface pointing to the files, which can load the data from disk on the fly when they are actually required.\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 551}, "executionInfo": {"elapsed": 1839, "status": "ok", "timestamp": 1646396782056, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "OzGm5f-qw8EL", "outputId": "f6f4bf23-e9ce-47d4-b23d-38b0868932de"}, "outputs": [], "source": ["# define image size\n", "IMG_HEIGHT = 20\n", "IMG_WIDTH = 200\n", "N_CHANNELS = 1\n", "N_CLASSES = 2\n", "\n", "# generator\n", "def hdf5_generator(path, buffer_size=32):\n", "    \"\"\" Load data INS data from disk\n", "    \n", "    Args:\n", "        path: path of the HDF5 file on disk\n", "        buffer_size: number of images to read from disk\n", "    \"\"\"\n", "    with h5py.File(path, 'r') as handle:\n", "        n_samples, h, w, c = handle['images'].shape\n", "        for i in range(0, n_samples, buffer_size):\n", "            images = handle['images'][i:i+buffer_size, ..., :1]\n", "            labels = handle['labels'][i:i+buffer_size]\n", "            yield images, labels\n", "\n", "# training data\n", "train_dataset = tf.data.Dataset.from_generator(lambda: hdf5_generator(path=join(data_path, 'ins-data/train.h5')), \n", "                                               output_types=(tf.float32, tf.float32),\n", "                                               output_shapes=((None, IMG_HEIGHT, IMG_WIDTH, N_CHANNELS), \n", "                                                              (None, N_CLASSES,)))\n", "\n", "# print\n", "print(train_dataset)\n", "# load the first buffer (with 32 data by default)\n", "images, labels = list(train_dataset.take(1))[0]\n", "\n", "# plot some images and labels from it\n", "nplot = 10\n", "fig, axes = plt.subplots(nplot // 2, 2, figsize=(16, nplot / 1.5), dpi=100)\n", "for ax, image, label in zip(axes.flatten(), images, labels):\n", "    ax.matshow(np.squeeze(image))\n", "    ax.set_xlabel('0: Dimer' if label[0] < .5 else '1: Goodenough', c='k')\n", "    ax.set_xticks([])\n", "    ax.set_yticks([])"]}, {"cell_type": "markdown", "metadata": {"id": "Zm4ClTDyw8EN"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "G2zGtQBnw8EN"}, "source": ["# 2. Build the network\n", "\n", "\n", "### $\\beta$-VAE for Image Generation\n", "\n", "This $\\beta$-VAE can be a combination of the CNN architecture and conventional VAE. \n", "\n", "Our image size is `(20, 20)`. The inputs and outputs should have the same size as the images. An additional hyperparameter $\\beta$ is introduced in a loss function. \n", "\n", "\n", "### Encoder and decoder\n", "\n", "First, we need to specify the latent dimension:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# latent dimension\n", "latent_dim = 32\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 174, "status": "ok", "timestamp": 1646396789887, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "bhaAlf94w8EN"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "0Ngdh9iaw8EO"}, "source": ["Now, extend the CNN to an encoder and a decoder. \n", "\n", "**Suggested Answer for Encoder** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# sampling z with (z_mean, z_log_var)\n", "class Sampling(keras.layers.Layer):\n", "    def call(self, inputs):\n", "        z_mean, z_log_var = inputs\n", "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n", "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n", "\n", "# build the encoder\n", "image_input = layers.Input(shape=(IMG_HEIGHT, IMG_WIDTH, N_CHANNELS))\n", "x = layers.Conv2D(8, kernel_size=(5, 5), activation='relu', padding='same')(image_input)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Conv2D(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPool2D(pool_size=(2, 2))(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Flatten()(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_BVAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "encoder_BVAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 650, "status": "ok", "timestamp": 1646396791596, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "ItQZedxIw8EO", "outputId": "29fd0f38-c9c3-4ffb-8737-23143feeb119"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "5jpK5M2d7ogl"}, "source": ["**Suggested Answer for Decoder** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the decoder\n", "z_input = layers.Input(shape=(latent_dim,))\n", "x = layers.Dense(800, activation=\"relu\")(z_input)\n", "x = layers.Reshape((2, 25, 16))(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.ZeroPadding2D((1, 0))(x)\n", "x = layers.Conv2DTranspose(16, kernel_size=(3, 3), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.UpSampling2D(size=(2, 2))(x)\n", "x = layers.Conv2DTranspose(8, kernel_size=(5, 5), activation='relu', padding='same')(x)\n", "x = layers.BatchNormalization()(x)\n", "image_output = layers.Conv2DTranspose(1, kernel_size=(3, 3), activation='linear', padding='same')(x)\n", "decoder_BVAE = keras.Model(z_input, image_output)\n", "decoder_BVAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 456, "status": "ok", "timestamp": 1646396794027, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "s47xYjHFw8EP", "outputId": "0cb1dbfd-7193-4af5-83b6-3bc22a2b1cf6"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "E4dfGc60w8EP"}, "source": ["### Training Loop\n", "\n", "The `BVAE` class can be the same as implemented in [VAE_basics.ipynb](VAE_basics.ipynb) except that we need to pass and use $\\beta$. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# BVAE class\n", "class BVAE(keras.Model):\n", "    # constructor\n", "    ########################################################\n", "    ######## NEW: passing beta as an extra argument ########\n", "    ########################################################\n", "    def __init__(self, encoder, decoder, beta, **kwargs):\n", "        super(BVAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "        self.beta = beta\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            # decoding\n", "            x_prime = self.decoder(z)\n", "            # reconstruction error by binary crossentropy loss\n", "            reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(x, x_prime))\n", "            reconstruction_loss *= IMG_HEIGHT * IMG_WIDTH\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            #######################################\n", "            ######## NEW: scale KL by beta ########\n", "            #######################################\n", "            loss = reconstruction_loss + self.beta * kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"beta_kl_loss\": self.beta * kl_loss}\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 4, "status": "ok", "timestamp": 1646396795732, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "V_l6qvnUw8EQ"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "GMdO2RQYw8EQ"}, "source": ["### Build and train the `BVAE` model\n", "\n", "Now, build the `BVAE` model and train it with the INS dataset. Let us first use $\\beta=5$ and start with 50 epochs. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the BVAE\n", "bvae_model = BVAE(encoder_BVAE, decoder_BVAE, beta=5.)\n", "\n", "# compile the BVAE\n", "bvae_model.compile(optimizer='adam')\n", "\n", "# train the BVAE\n", "training_history_BAVE = bvae_model.fit(train_dataset, epochs=50, batch_size=32)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 684507, "status": "ok", "timestamp": 1646397481521, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "DvGPnrtEw8ER", "outputId": "0ccc2d54-d5dd-43e6-911e-0d2bf79d7105"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "JdDaOg9uw8ER"}, "source": ["# 3. Analyse results \n", "\n", "Finally, we can generate new images using the decoder. After 50 epochs, the generated images resemble the original ones but look pretty vague. We can increase the definition by using more convolutional layers and a larger latent dimension (and thus more epochs) and by tuning the value of $\\beta$.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# generate images from the latent space\n", "def generate_images_latent(decoder, n_generation, feature_range):\n", "    # randomly sample the latent space\n", "    latent = []\n", "    for dim in range(latent_dim):\n", "        if len(np.array(feature_range).shape) == 1:\n", "            # only one range provided; used it for all dimensions\n", "            latent.append(np.random.uniform(feature_range[0], feature_range[1], \n", "                                            n_generation))\n", "        else:\n", "            # range provided for each dimension\n", "            latent.append(np.random.uniform(feature_range[dim][0], feature_range[dim][1], \n", "                                            n_generation))\n", "    latent = np.array(latent).T\n", "\n", "    # decode images\n", "    decodings = decoder.predict(latent)\n", "\n", "    # plot generated images\n", "    fig, axes = plt.subplots(n_generation // 2, 2, figsize=(16, n_generation / 2), dpi=100)\n", "    for ax, image in zip(axes.flatten(), decodings):\n", "        ax.matshow(image[:, :, 0])\n", "        ax.set_xticks([])\n", "        ax.set_yticks([])\n", "    plt.show()  \n", "\n", "# generate random images sampled between [-1, 1]\n", "generate_images_latent(decoder_BVAE, n_generation=30, feature_range=[-1, 1])\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"elapsed": 2496, "status": "ok", "timestamp": 1646397484010, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "TAinw3rgw8ER", "outputId": "2dd1b5d2-0036-4144-d7a4-12d79ac7a6e5"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "htmuMNvew8ES"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "i_pu65anw8ES"}, "source": ["# 4. Exercises:\n", "\n", "1. Tune `latent_dim` and `beta` (and use more epochs) to improve the quality of image generation.\n", "2. Implement a conditional VAE for this INS dataset."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "DHB2jU5_w8ES"}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "Material_VAE inelastic neutron scattering_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}