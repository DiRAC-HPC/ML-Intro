{"cells": [{"cell_type": "markdown", "metadata": {"id": "2oQO79YyqUFI"}, "source": ["# 0. Autoencoder and Variational Autoencoder feature extraction for Regression in Equilibrium Astrochemistry \n", "\n", "In this notebook, we attempt to build an autoencoder and a regression model to extract important features from chemical abundances and use encodered features to predict gas temeperature and chemical abundances times serieses.\n", "\n", "Since most gas cooling in astrophysical contexts comes from molecules radiating energy away, their abundances essentially set the cooling rate. Since the temperature affects how the gas evolves dynamically, any physics simulation of processes such as star formation requires a method of accurately calculating the chemistry of the gas.\n", "\n", "Unfortunately, accurately calculating the temperature and chemistry (henceforth thermochemistry) of a gas can be computationally expensive. Many approaches such as simplifying chemical networks to only the most important species and reactions or assuming equilibrium are used to decrease the cost of these calculations. However, even these can fail for large simulations where an object is broken up into many parts which all require a thermochemical calculation.\n", "\n", "The aim of this work is to use Autoencoders and Variational autoencoders for feature extraction and then to solve the chemistry and gas temperature with sufficient accuracy and speed to render a host of new problems attainable. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 7768, "status": "ok", "timestamp": 1646401913225, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "dkntd1jNqUFM", "outputId": "ac35290e-aa1a-41d0-c448-0165749e48e0"}, "outputs": [], "source": ["# tensorflow\n", "from tensorflow.keras.layers import Input, Dense, Multiply, Add\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras import Sequential\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "import tensorflow as tf\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from os.path import join\n", "import seaborn as sns\n", "from glob import glob\n", "from tqdm import tqdm\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n"]}, {"cell_type": "markdown", "metadata": {"id": "HL61bPFxqk4N"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 17531, "status": "ok", "timestamp": 1646401930752, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "mZxI7J_Cqlzz"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 19993, "status": "ok", "timestamp": 1646401950741, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "gw2CflHBqnYU", "outputId": "4a9ae7dd-ea29-4d09-eb0b-69e831283e81"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "1ovtPK55vUyC"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "RjGoKV2SqUFP"}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "We start by loading the dataset taken from [Chemulator - Holdship et al. 2020](https://ui.adsabs.harvard.edu/abs/2021A%26A...653A..76H/abstract).\n", "\n", "The full **Chemulator** dataset contains the abundances and physical parameters as a function of time when exposed to different conditions. The physical parameters are:\n", "- Gas temperature \n", "- Gas density\n", "- Radation Field \n", "- Local cosmic ray ionization rate \n", "- Ratio of the abundance of metals in this gas to the sollar abundance of metal \n", "- Total Column density\n", "- $H_2$ Column density\n", "- $C$ Column density\n", "\n", "and the abundances are:\n", "\n", "'H','H+', 'H2', 'H2+', 'H3+', 'HE', 'HE+', 'C+', 'C', 'CH', 'CH+', 'CH2', 'CH2+', 'CH3+', 'CH3', 'CH4+', 'O+', 'CH4', 'O', 'CH5+', 'OH+', 'OH', 'H2O+', 'H2O', 'H3O+', 'MG', 'MG+', 'CO+', 'CO', 'HCO+', 'O2', 'O2+','E-'\n", "\n", "The data are stored in the hdf file 'NHS/small_chemulator_dataset_train.hdf' and 'NHS/small_chemulator_dataset_test.hdf'.\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "train_data = pd.read_hdf(join(data_path, 'NHS/small_chemulator_dataset_train.hdf'), 'df')  \n", "test_data = pd.read_hdf(join(data_path, 'NHS/small_chemulator_dataset_test.hdf'), 'df')  \n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 19217, "status": "ok", "timestamp": 1646401969954, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "jK6-hCtlqUFQ"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 17685, "status": "ok", "timestamp": 1646401987635, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "aXd5H8NDqUFR"}, "outputs": [], "source": ["class Scale_info:\n", "    def __init__(self, data):\n", "        self.min = np.min(data, axis = 0)\n", "        self.max = np.max(data, axis = 0)\n", "\n", "def preprocessing(df, data_type = 'chemical'):\n", "    arr = df.copy()\n", "    arr = np.log10(arr)\n", "    if data_type == 'chemical':\n", "        arr=np.where(arr<-20,-20,arr)\n", "    arr = arr.astype(np.float32)\n", "    reliable_min = np.percentile(arr, 2, axis =0)\n", "    reliable_max = np.percentile(arr, 98, axis =0)\n", "\n", "    arr = (arr - reliable_min)/(reliable_max - reliable_min)\n", "    arr = np.clip(arr, 0., 1.)\n", "    \n", "    return arr # array\n", "\n", "abundances = ['H','H+', 'H2', 'H2+', 'H3+', 'HE', 'HE+', 'C+', 'C', 'CH', 'CH+', 'CH2',\n", "       'CH2+', 'CH3+', 'CH3', 'CH4+', 'O+', 'CH4', 'O', 'CH5+', 'OH+', 'OH',\n", "       'H2O+', 'H2O', 'H3O+', 'MG', 'MG+', 'CO+', 'CO', 'HCO+', 'O2', 'O2+','E-']\n", "\n", "\n", "### Preprocessing chemical abundances\n", "train_data['HE'] = 0.1*train_data['metallicity']\n", "chemical_abundances = train_data[abundances]\n", "chemical_abundances = preprocessing(chemical_abundances, data_type = 'chemical')\n", "train_data[abundances] = chemical_abundances\n", "\n", "test_data['HE'] = 0.1*test_data['metallicity']\n", "chemical_abundances_test = test_data[abundances]\n", "chemical_abundances_test = preprocessing(chemical_abundances_test, data_type = 'chemical')\n", "test_data[abundances] = chemical_abundances_test\n", "\n", "\n", "### Preprocessing physical parameters\n", "physics_labels=['gas_temp','gas_density','radfield','zeta','coldens','h2col','ccol',\"metallicity\"]\n", "train_data[physics_labels] = preprocessing(train_data[physics_labels], data_type = 'physical')\n", "test_data[physics_labels] = preprocessing(test_data[physics_labels], data_type = 'physical')"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 1052, "status": "ok", "timestamp": 1646401988684, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "UdBqityaqUFR", "outputId": "f66bd6ff-38dc-490b-806f-477e680d2e42"}, "outputs": [], "source": ["train_all = np.array(train_data[physics_labels + abundances])[:-1]\n", "train_all_target = np.array(train_data[['gas_temp']+abundances])[1:]\n", "test_all = np.array(test_data[physics_labels + abundances])[:-1]\n", "test_all_target = np.array(test_data[['gas_temp']+abundances])[1:]\n", "\n", "print(train_all.shape)\n", "print(train_all_target.shape)\n", "print(test_all.shape)\n", "print(test_all_target.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "j7KFlSxkqUFS"}, "source": ["# 2. Build the network\n", "\n", "### Regression using all abundances\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model_all = Sequential()\n", "model_all.add(Input(shape=(train_all.shape[1],)))\n", "\n", "for layer in [256, 256]:\n", "    model_all.add(Dense(layer, activation=\"relu\"))\n", "\n", "model_all.add(Dense(1+len(abundances),activation='sigmoid'))        \n", "\n", "model_all.compile(loss=\"mse\", optimizer='adam')\n", "\n", "stopper=EarlyStopping( monitor =\"val_loss\", min_delta=0.0,verbose=1,\n", "                      mode=\"min\",\n", "                      restore_best_weights=True,patience=2)\n", "history_all = model_all.fit(train_all, train_all_target, epochs=5, batch_size=128, \n", "                             validation_data = (test_all, test_all_target) ,callbacks=[stopper])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 443902, "status": "ok", "timestamp": 1646402432582, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "gXpPLME9qUFS", "outputId": "b7e5884a-58ef-4735-e267-e4177b9c505e"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results"]}, {"cell_type": "markdown", "metadata": {"id": "0RZHYxnQscZM"}, "source": ["### Take a look at some results\n", "\n", "Using predict_multiple_timesteps function we can predict abundances and gas temperature time series from their initial valeus."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 18, "status": "ok", "timestamp": 1646402432584, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "CqI7A8MTqUFT"}, "outputs": [], "source": ["def predict_multiple_timesteps(model, init_condition, time = 999):\n", "    x = init_condition.copy()\n", "    predictions = [x.copy()]\n", "    for i in tqdm(range(time)):\n", "        pred = model.predict(x)\n", "        x[:, 0] = pred[:, 0]\n", "        x[:, len(physics_labels):] = pred[:, 1:]\n", "        predictions.append(x.copy())\n", "    return predictions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Predict multiple timesteps\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "init_idx = 0\n", "init_condition = train_all[init_idx:init_idx+1]\n", "pred_all = predict_multiple_timesteps(model_all, init_condition, time = 999)\n", "\n", "pred_all = np.stack(pred_all).reshape(1000, train_all.shape[1])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"elapsed": 71360, "status": "ok", "timestamp": 1646402815732, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "qW0mi5fiqUFT", "outputId": "d278d9c3-7364-4190-c8b0-10a62182766d"}, "outputs": [], "source": ["plt.figure()\n", "plt.plot(train_all_target[init_idx:init_idx+1000, 0], label = 'target')\n", "plt.plot(pred_all[:, 0], label = 'prediction')\n", "plt.axis([-1., 1001., -0.05, 1.05])\n", "plt.title('gas')\n", "plt.legend(loc = 1)\n", "for i in range(len(abundances)):\n", "    plt.figure()\n", "    plt.plot(train_all_target[init_idx:init_idx+1000, 1+i], label = 'target')\n", "    plt.plot(pred_all[:, len(physics_labels)+i], label = 'prediction')\n", "    plt.axis([-1., 1001., -0.05, 1.05])\n", "    plt.title(abundances[i])\n", "    plt.legend(loc = 1)"]}, {"cell_type": "markdown", "metadata": {"id": "5QZACCnUqUFU"}, "source": ["# 4. Regression using features extracted by Autoencoders"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 293, "status": "ok", "timestamp": 1646402816014, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "tvDuOjbkqUFU"}, "outputs": [], "source": ["train_abundances = np.array(train_data[abundances])\n", "test_abundances = np.array(test_data[abundances])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Build AE with 8 dimensional latent space\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "encoding_dim = 8  \n", "\n", "# This is our input image\n", "inputs = Input(shape=(len(abundances),))\n", "en_hidden = Dense(256, activation='swish')(inputs)\n", "encoded = Dense(encoding_dim)(en_hidden)\n", "\n", "de_hidden = Dense(256, activation='swish')(encoded)               \n", "decoded = Dense(len(abundances))(de_hidden)\n", "\n", "# This model maps an input to its reconstruction\n", "AE = Model(inputs, decoded)\n", "AE_encoder = Model(inputs, encoded)\n", "\n", "AE.compile(loss=\"mse\", optimizer='adam')\n", "\n", "stopper=EarlyStopping(monitor =\"val_loss\", min_delta=0.0,verbose=1,\n", "                      mode=\"min\",\n", "                      restore_best_weights=True,patience=2)\n", "history_ae = AE.fit(train_abundances, train_abundances, epochs=5, batch_size=128, \n", "                             validation_data = (test_abundances, test_abundances) ,callbacks=[stopper])```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 262985, "status": "ok", "timestamp": 1646403078993, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "eWVJFg2NqUFU", "outputId": "a799da73-4bfb-473a-dbff-05875d671e0a"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "L-kGOjTWs6og"}, "source": ["### Visualize reconstruction results using the autoencoder\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "test_recon = AE.predict(test_abundances)\n", "idx_abundances = 0\n", "plt.figure()\n", "plt.plot(test_abundances[:, idx_abundances], label = 'true ' + abundances[idx_abundances])\n", "plt.plot(test_recon[:, idx_abundances], label = 'reconstruction')\n", "plt.legend(bbox_to_anchor=(1., 1.03))\n", "plt.xlabel('time')\n", "plt.ylabel(abundances[idx_abundances], rotation=0)\n", "plt.title(abundances[idx_abundances] + ' reconstruction result')\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 312}, "executionInfo": {"elapsed": 22317, "status": "ok", "timestamp": 1646403101874, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "2mq-rc-PqUFU", "outputId": "3c500ede-d610-44d0-ce53-d7182b1eb400"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "Yamj5_zKtB6R"}, "source": ["### Prepare encoded features using the encoder of the autoencoder"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 249342, "status": "ok", "timestamp": 1646403351205, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Oki5Gd6WqUFV", "outputId": "ab39f70a-425c-4e25-d4d7-7d879bdeb5d5"}, "outputs": [], "source": ["abundances_encoded = []\n", "for i in tqdm(range(int(len(train_data)/1000))):\n", "    abundances_encoded.append(AE_encoder.predict(np.array(train_abundances[i*1000:(i+1)*1000])))\n", "    \n", "abundances_test_encoded = []\n", "for i in tqdm(range(int(len(test_data)/1000))):\n", "    abundances_test_encoded.append(AE_encoder.predict(np.array(test_abundances[i*1000:(i+1)*1000])))\n", "    \n", "abundances_encoded = np.concatenate(abundances_encoded, axis =0)\n", "abundances_test_encoded = np.concatenate(abundances_test_encoded, axis =0)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 888, "status": "ok", "timestamp": 1646403352081, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "XfOPbitCqUFV", "outputId": "06812e5f-525b-4c54-9500-71449ec78a83"}, "outputs": [], "source": ["ae_scale = Scale_info(abundances_encoded)\n", "abundances_encoded = (abundances_encoded-ae_scale.min)/(ae_scale.max - ae_scale.min)\n", "\n", "train_ae = np.concatenate((np.array(train_data[physics_labels]), abundances_encoded), axis = -1)[:-1]\n", "train_ae_target = np.concatenate((np.array(train_data[['gas_temp']]), abundances_encoded), axis = -1)[1:]\n", "\n", "test_ae = np.concatenate((np.array(test_data[physics_labels]), abundances_test_encoded), axis = -1)[:-1]\n", "test_ae_target = np.concatenate((np.array(test_data[['gas_temp']]), abundances_test_encoded), axis = -1)[1:]\n", "\n", "print(train_ae.shape)\n", "print(train_ae_target.shape)\n", "print(test_ae.shape)\n", "print(test_ae_target.shape)"]}, {"cell_type": "markdown", "metadata": {"id": "rL0-Gu71tI9P"}, "source": ["### Train a regression model from encoded featrues and gas temperature\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model_ae = Sequential()\n", "model_ae.add(Input(shape=(train_ae.shape[1],)))\n", "\n", "for layer in [256, 256]:\n", "    model_ae.add(Dense(layer, activation=\"relu\"))\n", "\n", "model_ae.add(Dense(1+encoding_dim,activation='sigmoid'))        \n", "\n", "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n", "model_ae.compile(loss=\"mse\", optimizer=opt)\n", "\n", "stopper=EarlyStopping(monitor =\"val_loss\", min_delta=0.0,verbose=1,\n", "                      mode=\"min\",\n", "                      restore_best_weights=True,patience=2)\n", "history_all = model_ae.fit(train_ae, train_ae_target, epochs=5, batch_size=128, \n", "                             validation_data = (test_ae, test_ae_target) ,callbacks=[stopper])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 368420, "status": "ok", "timestamp": 1646403720491, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "YMJXwNAHqUFV", "outputId": "0e345ef6-fe81-4660-e88c-f17a747fe951"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "wjWg4kqQtUFG"}, "source": ["### Take a look at some results\n", "\n", "Again, using predict_multiple_timesteps function we can predict abundances and gas temperature time series from their initial valeus."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"elapsed": 67070, "status": "ok", "timestamp": 1646403945440, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "6FKGmkn3qUFW", "outputId": "cd299071-8f6f-41c6-d00b-987e52db6ca3"}, "outputs": [], "source": ["init_idx = 0\n", "\n", "init_condition_part = train_ae[init_idx:init_idx+1]\n", "pred_ae = predict_multiple_timesteps(model_ae, init_condition_part, time = 999)\n", "\n", "pred_ae = np.stack(pred_ae).reshape(1000, train_ae.shape[1])\n", "\n", "pred_ae[:, len(physics_labels):] = (ae_scale.max - ae_scale.min)*pred_ae[:, len(physics_labels):] +ae_scale.min\n", "\n", "de_hidden = AE.layers[3](pred_ae[:, len(physics_labels):])\n", "recon_abundances = AE.layers[4](de_hidden)\n", "\n", "pred_ae = np.concatenate((pred_ae[:, :len(physics_labels)], recon_abundances), axis = -1)\n", "\n", "plt.figure()\n", "plt.plot(train_all_target[init_idx:init_idx+1000, 0], label = 'target')\n", "plt.plot(pred_ae[:, 0],  label = 'prediction')\n", "plt.axis([-1., 1001., -0.05, 1.05])\n", "plt.title('gas')\n", "plt.legend(loc = 1)\n", "for i in range(len(abundances)):\n", "    plt.figure()\n", "    plt.plot(train_all_target[init_idx:init_idx+1000, 1+i], label = 'target')\n", "    plt.plot(pred_ae[:, len(physics_labels)+i], label = 'prediction')\n", "    plt.axis([-1., 1001., -0.05, 1.05])\n", "    plt.title(abundances[i])\n", "    plt.legend(loc = 1)"]}, {"cell_type": "markdown", "metadata": {"id": "QHiOlf5hqUFW"}, "source": ["# 5. Exercise\n", "\n", "### Feature extraction using VAE\n", "\n", "In this notebook, we used Autoencoders to extracted features and train neural networks with encoded abundances.  \n", "Use a Variational Autoencoders to extract more meaningful features.\n", "\n", "- Can you significantly improve the performance?\n", "- Do we need to use $\\beta$-vae to balance between reconstruction loss and regulariser?\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "SHD7hTZQqUFW"}, "outputs": [], "source": []}], "metadata": {"colab": {"collapsed_sections": [], "name": "NHS_AE feature extraction for regression_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}