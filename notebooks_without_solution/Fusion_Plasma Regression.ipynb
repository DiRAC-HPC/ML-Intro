{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["\n", "# 0. The evolution of plasma state prediction\n", "\n", "In this notebook, we attempt to build a Long short-term memory (LSTM) model to predict time series.\n", "\n", "Thomson scattering is an important tool for plasma diagnostics in nuclear fusion facilities. Evolution of the Thomson Scattering data mapping the electron temperature and density of the profiles are predicted using an LSTM model. The model takes in the first 8 time instances of the profile and then maps the next 4 time instances. \u200b\n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow.keras import layers\n", "from tensorflow.keras.layers import Input, Dense\n", "from tensorflow.keras.models import Model\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import LSTM\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import matplotlib\n", "from collections import Counter\n", "from scipy import asarray as ar,exp\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import mixture\n", "from sklearn import cluster\n", "from sklearn.decomposition import PCA\n", "import pandas as pd\n", "import os.path\n", "import matplotlib.patches as mpatches\n", "from sklearn.cluster import AgglomerativeClustering\n", "from sklearn.cluster import MeanShift\n", "import os\n", "from IPython.display import clear_output\n", "from tqdm import tqdm\n", "import h5py\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n", "\n", "data_path = 'data'\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Load the dataset\n", "\n", "### Read in the Thomson scattering data\n", "\n", "Use pandas to read the hdf 'Fusion/fusion_inputs.h5' and 'Fusion/fusion_targets.h5'. 'fusion_inputs.h5' includes six time series data: plasma current, toroidal mag field, z position, aspect ratio (major radius/minor radius), gas outboard and aux heat. 'fusion_targets.h5' includes electron density and electron temperature. All time series follow the same times and shot number \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "with h5py.File(join(data_path, 'Fusion/fusion_inputs.h5'), 'r') as F:\n", "\n", "    current_data = {}\n", "    for i in range(len(np.array(F['current_data']))):\n", "        current_data[i] = np.array(F['current_data'][str(i)])\n", "        \n", "    mag_data = {}\n", "    for i in range(len(np.array(F['mag_data']))):\n", "        mag_data[i] = np.array(F['mag_data'][str(i)])\n", "    \n", "    Z_data = {}\n", "    for i in range(len(np.array(F['Z_data']))):\n", "        Z_data[i] = np.array(F['Z_data'][str(i)])\n", "    \n", "    aspect_data = {}\n", "    for i in range(len(np.array(F['aspect_data']))):\n", "        aspect_data[i] = np.array(F['aspect_data'][str(i)])\n", "\n", "    gas_data = {}\n", "    for i in range(len(np.array(F['gas_data']))):\n", "        gas_data[i] = np.array(F['gas_data'][str(i)])\n", "\n", "    heat_data = {}\n", "    for i in range(len(np.array(F['heat_data']))):\n", "        heat_data[i] = np.array(F['heat_data'][str(i)])\n", "        \n", "        \n", "\n", "with h5py.File(join(data_path, 'Fusion/fusion_targets.h5'), 'r') as F:\n", "\n", "    shot_num = {}\n", "    for i in range(len(np.array(F['shot_num']))):\n", "        shot_num[i] = int(np.array(F['shot_num'][str(i)]))\n", "        \n", "    time_data = {}\n", "    for i in range(len(np.array(F['time_data']))):\n", "        time_data[i] = np.array(F['time_data'][str(i)])\n", "    \n", "    density_data = {}\n", "    for i in range(len(np.array(F['density_data']))):\n", "        density_data[i] = np.array(F['density_data'][str(i)])\n", "    \n", "    temperature_data = {}\n", "    for i in range(len(np.array(F['temperature_data']))):\n", "        temperature_data[i] = np.array(F['temperature_data'][str(i)])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["e_temp = temperature_data\n", "e_density = density_data"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### combine signals into one stacked array for each"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["current_stack = np.hstack(list(current_data.values()))\n", "mag_stack = np.hstack(list(mag_data.values()))\n", "Z_stack = np.hstack(list(Z_data.values()))\n", "aspect_stack = np.hstack(list(aspect_data.values()))\n", "gas_stack = np.hstack(list(gas_data.values()))\n", "heat_stack = np.hstack(list(heat_data.values()))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Normalise each signal array by their maximum value\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "current_stack = current_stack/np.max(current_stack)\n", "mag_stack = mag_stack/np.max(mag_stack)\n", "Z_stack = Z_stack/np.max(Z_stack)\n", "aspect_stack = aspect_stack/np.max(aspect_stack)\n", "gas_stack = gas_stack/np.max(gas_stack)\n", "heat_stack = heat_stack/np.max(heat_stack)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Combine all signals"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["all_signals = np.column_stack((current_stack,mag_stack,Z_stack,aspect_stack,gas_stack,heat_stack))\n", "np.shape(all_signals)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Create array of radial profiles (temperature and density) across all shots"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["temp_flat = []\n", "for i in range(len(e_temp)):\n", "    for j in range(len(e_temp[i])):\n", "        temp_flat.append(e_temp[i][j])\n", "        \n", "density_flat = []\n", "for i in range(len(e_density)):\n", "    for j in range(len(e_density[i])):\n", "        density_flat.append(e_density[i][j])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reshape array of radial profiles into 2D temporal/radial arrays for each shot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["e_temp_reshaped = []\n", "e_density_reshaped = []\n", "b = []\n", "a = 0\n", "for i in range(len(e_temp)):\n", "    b.append(a)\n", "    b_sum = np.sum(b)\n", "    a = len(e_temp[i])\n", "    e_temp_reshaped.append(temp_flat[b_sum:a+b_sum])\n", "    e_density_reshaped.append(density_flat[b_sum:a+b_sum])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Combine density and temperature"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_temp = temp_flat/np.max(temp_flat) # normalise\n", "new_density = density_flat/np.max(density_flat)\n", "new_temp = np.reshape(new_temp,(len(new_temp),110,1))\n", "new_density = np.reshape(new_density,(len(new_density),110,1))\n", "target_all_flat = np.concatenate([new_temp,new_density],axis=2)\n", "print(np.shape(target_all_flat))\n", "training_shots_all = target_all_flat.reshape(len(target_all_flat),220)\n", "print(np.shape(training_shots_all))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def split_sequences(sequences, n_steps):\n", "    X, y = list(), list()\n", "    for i in range(len(sequences)):\n", "        # find the end of this pattern\n", "        end_ix = i + n_steps\n", "        # check if we are beyond the dataset\n", "        if end_ix > len(sequences)-1:\n", "            break\n", "        # gather input and output parts of the pattern\n", "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix, :]\n", "        X.append(seq_x)\n", "        y.append(seq_y)\n", "    return np.array(X), np.array(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Split a multivariate sequence into samples using split_sequences\n", "\n", "We will have a gap for a furture event prediction. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["n_steps = 8\n", "X, x1 = split_sequences(all_signals,n_steps) # X = training data, all_signals or all_signals_flat_test\n", "print(np.shape(X))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## delete end rows of training array depending on gap size\n", "gap = 4\n", "X = X[:-gap]\n", "print(np.shape(X))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## make target e_temp array \n", "e_all_target = training_shots_all[n_steps+gap:] ## e_all_flat or training_shots_all\n", "y = e_all_target.reshape(len(e_all_target),220) # use if training on all data\n", "y = e_all_target ## use for surrogate dataset\n", "print(np.shape(y))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Build the network\n", "\n", "In this case, the inputs are time series of six features and outputs are electron density and electron temperature. We suggest to build a neural network using dense and lstm layers. In order to stack LSTM layers, return_sequences should be True.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "n_features = X.shape[2]\n", "\n", "# define model\n", "model = Sequential()\n", "model.add(Dense(n_features, activation='relu', input_shape=(n_steps, n_features)))\n", "model.add(Dense(n_features, activation='relu'))\n", "model.add(LSTM(12, activation='relu', return_sequences=True))\n", "model.add(layers.Dropout(0.2))\n", "model.add(LSTM(24, activation='relu', return_sequences=True))\n", "model.add(layers.Dropout(0.2))\n", "model.add(LSTM(48, activation='relu', return_sequences=True))\n", "model.add(layers.Dropout(0.2))\n", "model.add(LSTM(100, activation='relu', return_sequences=True))\n", "model.add(layers.Dropout(0.2))\n", "model.add(LSTM(200, activation='relu'))\n", "model.add(layers.Dropout(0.2))\n", "model.add(Dense(220))\n", "model.compile(optimizer='adam', loss='mse')\n", "print(model.summary())\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the model with EarlyStopping\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# fit model\n", "stopper=EarlyStopping( monitor =\"val_loss\", min_delta=0.0,verbose=1,\n", "                      mode=\"min\",\n", "                      restore_best_weights=True,patience=2)\n", "\n", "history = model.fit(X, y, epochs=50, batch_size=256, shuffle=True, validation_split=1/12.,callbacks=[stopper])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results \n", "\n", "### Check training history\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# loss curves\n", "plt.figure(figsize=(8,5))\n", "plt.plot(history.history['loss'],linewidth=2.5)\n", "plt.plot(history.history['val_loss'],linewidth=2.5)\n", "plt.ylabel('Loss value',fontsize=14)\n", "plt.xlabel('Epoch',fontsize=14)\n", "plt.xticks(fontsize=14)\n", "plt.yticks(fontsize=14)\n", "plt.legend(['Training loss', 'Validation loss'], loc='upper right',fontsize=14)\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Predict and compare errors\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "y_pred = model.predict(X)\n", "print(np.shape(y_pred))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## flattened array for comparison\n", "\n", "e_all_compare = e_all_target[n_steps+gap:]\n", "e_all_compare = e_all_compare.reshape(len(e_all_compare),220)\n", "np.shape(e_all_compare)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## compare to original decoded representation\n", "\n", "errors = []\n", "errors_t = []\n", "errors_d = []\n", "\n", "for i in range(0,len(e_all_compare)):\n", "    subtract = (e_all_target[i] - y_pred[i])**2\n", "    subtract_reshaped = subtract.reshape(110,2)\n", "    \n", "    mse = np.mean(subtract)\n", "    subtract_mse_t = np.mean(subtract_reshaped[:,0])\n", "    subtract_mse_d = np.mean(subtract_reshaped[:,1]) ## mse for density and temp\n", "    \n", "    errors.append(mse)\n", "    errors_t.append(subtract_mse_t)\n", "    errors_d.append(subtract_mse_d)\n", "    \n", "plt.figure(figsize=(15,10))\n", "plt.plot(errors)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Display the mean of errors of temeprature and density\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "print(np.mean(errors_t)*100,'%') ##temp\n", "print(np.mean(errors_d)*100,'%') ##density\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reshape array of predicted radial profiles into 2D temporal/radial arrays for each shot"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = y_pred.reshape(len(y_pred),110,2)\n", "\n", "temp_pred_reshaped = []\n", "density_pred_reshaped = []\n", "b = []\n", "b1 = len(e_temp[0])-(n_steps+gap)\n", "temp_pred_reshaped.append(y_pred[0:b1,:,0]*np.max(temp_flat)) ## *np.max(e_temp_flat) to un-normalise\n", "density_pred_reshaped.append(y_pred[0:b1,:,1]*np.max(density_flat))\n", "b.append(b1)\n", "a = 0\n", "for i in tqdm(range(1,len(e_temp))):\n", "    b.append(a)\n", "    b_sum = np.sum(b)\n", "    a = len(e_temp[i])\n", "    temp_pred_reshaped.append(y_pred[b_sum:a+b_sum,:,0]*np.max(temp_flat)) \n", "    density_pred_reshaped.append(y_pred[b_sum:a+b_sum,:,1]*np.max(density_flat)) "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## compare predicted to original radial profiles (temperature)\n", "\n", "# time vs. radius\n", "from random import randrange\n", "shot_index = randrange(len(shot_num))\n", "print('shot number = ',shot_num[shot_index])\n", "\n", "fig = plt.figure(figsize=(12,8))\n", "fig.add_subplot(1,2,1)\n", "plt.imshow(e_temp_reshaped[shot_index],vmin=0,vmax=np.max(temp_pred_reshaped[shot_index]),origin='lower')\n", "plt.title('Shot '+str(shot_num[shot_index]))\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Temperature [eV]')\n", "\n", "fig.add_subplot(1,2,2)\n", "plt.subplots_adjust(wspace=0.25)\n", "plt.imshow(temp_pred_reshaped[shot_index],vmin=0,vmax=np.max(temp_pred_reshaped[shot_index]),origin='lower')\n", "plt.title('Predicted Shot '+str(shot_num[shot_index]))\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Temperature [eV]')\n", "\n", "## compare predicted to original radial profiles (density)\n", "\n", "# time vs. radius\n", "\n", "fig = plt.figure(figsize=(12,8))\n", "fig.add_subplot(1,2,1)\n", "plt.imshow(e_density_reshaped[shot_index],vmin=0,vmax=np.max(density_pred_reshaped[shot_index]),origin='lower')\n", "plt.title('Shot '+str(shot_num[shot_index]))\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Density [m-3]')\n", "\n", "fig.add_subplot(1,2,2)\n", "plt.imshow(density_pred_reshaped[shot_index],vmin=0,vmax=np.max(density_pred_reshaped[shot_index]),origin='lower')\n", "plt.title('Predicted Shot '+str(shot_num[shot_index]))\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Density [m-3]')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Exercises\n", "\n", "### Let's try doing some hyper-parameter tuning.  \n", "\n", "* Different step sizes for time series data\n", "* Different gap sizes\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 4}