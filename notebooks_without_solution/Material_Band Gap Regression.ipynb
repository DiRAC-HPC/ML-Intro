{"cells": [{"cell_type": "markdown", "metadata": {"id": "V7Cvlxgi592j"}, "source": ["# 0. Band gap regression using Decision Trees\n", "\n", "In this notebook, we will use decision trees to solve regression problems. \n", "\n", "The dataset used here originates from a project to build a surrogate model for predicting the band gap of a material from its composition. This surrogate model was used to replace expensive qunatum mecahnical calculations in virtual high-throughput screening of materials for application as photocatalysts. The paper was published in [Chemistry of Materials](https://pubs.acs.org/doi/abs/10.1021/acs.chemmater.9b01519). \n", "\n", "The aim of this work is to use regression trees and learn how to tune hyperparameters for best performance."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 1339, "status": "ok", "timestamp": 1646397314588, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "dNmwpmyL592o"}, "outputs": [], "source": ["# sklearn\n", "from sklearn import metrics\n", "from sklearn.linear_model import LinearRegression\n", "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n", "from sklearn.ensemble import GradientBoostingRegressor\n", "import sklearn.datasets\n", "\n", "# helpers\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n"]}, {"cell_type": "markdown", "metadata": {"id": "W5TzJfq12gf1"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 12924, "status": "ok", "timestamp": 1646397327505, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "bJiBOWNO2hrV"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 19444, "status": "ok", "timestamp": 1646397348977, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "nDlYhH-m2l2q", "outputId": "7ba139da-0814-40ce-ce98-c1383cf1fc44"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "pZoCYPA6592p"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "BR6-gofe592p"}, "source": ["# 1. Load the dataset\n", "\n", "Our data are stored in the pickle file `Material/oxides.pickle`. We load this file into a `pandas.DataFrame` object, an efficient interface to manage column-wise, heterogeneous tabular data. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "oxides = pd.read_pickle(join(data_path, 'Material/oxides.pickle'))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 3, "status": "ok", "timestamp": 1646397348977, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "zPuKWTyQ592q"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "IIU5wgz6592q"}, "source": ["We can check all the columns presented in the dataframe:"]}, {"cell_type": "markdown", "metadata": {"id": "_UFc_h5C592q"}, "source": ["To read data from one of the columns, use the `values` attribute, for example:"]}, {"cell_type": "markdown", "metadata": {"id": "mTVjKO16592r"}, "source": ["### Description of the dataset\n", "\n", "In this practical we are attempting to learn a model that can predict the band gap (energy separation between occupied and un-occupied orbitals) of a material. So we need to set this value as the property to be predicted $y$ This data is stored in the dataframe column called `gllbsc_gap` and we set this to be y by running the cell below:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# read a single column\n", "y = oxides['gllbsc_gap'].values\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 228, "status": "ok", "timestamp": 1646397349203, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "S-NlgI0h592r"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "UYiecYOA592s"}, "source": ["We can then use the other properties in the dataset, or a combination of them as *features* ($X$) for our model. For example we could set X to be defined by two features by running the cell below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 15, "status": "ok", "timestamp": 1646397349204, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "WxsBZEaq592s", "outputId": "db4b0225-a48a-49c0-93d7-a49bd5e8b837"}, "outputs": [], "source": ["# read multiple columns and combine them to a matrix\n", "X = oxides[['MagpieData minimum Number', 'MagpieData maximum Number']].values\n", "print(X.shape)"]}, {"cell_type": "markdown", "metadata": {"id": "4Ad5iW_w592t"}, "source": ["# 2. Build a baseline model\n", "\n", "In regression, we attempt to fit a model, $y = f(x)$, where $x$ and $y$ are multi-dimensional data of rank $M$ and $N$, respectively, and $f: \\mathbb{R}^M\\rightarrow\\mathbb{R}^N$ our regression model. In this notebook, $y$ will always be `gllbsc_gap` (so $N=1$), which represents the band gap, and $x$ a combination of the descriptors (all the other columns), each giving the measurement of a certain physical property. "]}, {"cell_type": "markdown", "metadata": {"id": "3j8uAZ2P592t"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "buW_FBkF592t"}, "source": ["## Linear regression: a starter\n", "\n", "Linear regression is the simplest regression algorithm in machine learning. Many people do not even regard it as a machine learning algorithm because it is explicitly programmed. Still, it serves as a good start to learn some basic concepts.\n", "\n", "\n", "## Univariate regression\n", "\n", "In univariate linear regression we have the equation:\n", "$y = mx + c$\n", "and we are attempting to find the best values for $m$ and $c$\n", "\n", "In a univariate regression, the input rank $M=1$. For instance, let us try `MagpieData avg_dev Electronegativity` as $x$:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# read X\n", "X = oxides['MagpieData avg_dev Electronegativity'].values\n", "# we need to append a dummy dimension to X for univariate regression\n", "# to keep the input dimensions consistent with multivariate regression\n", "X = X.reshape(-1, 1)\n", "# read y\n", "y = oxides['gllbsc_gap'].values\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 9, "status": "ok", "timestamp": 1646397349204, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "EVwlbp_l592u"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "WNo5uFNO592u"}, "source": ["Now we can use linear regression to fit the data and make predictions:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 8, "status": "ok", "timestamp": 1646397349204, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "ly9tHzhw592u"}, "outputs": [], "source": ["# fit linear regression model\n", "model = LinearRegression().fit(X, y)\n", "# make predictions\n", "y_pred = model.predict(X)"]}, {"cell_type": "markdown", "metadata": {"id": "1ibFDJzV592u"}, "source": ["When we have fitted the model we now want to use some *metrics* to *evaluate* the model performance. Remember the mean squared error and mean absolute error from your lectures. We will now calculate them for the model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 8, "status": "ok", "timestamp": 1646397349205, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Y14i8wMS592v", "outputId": "14807eeb-fe82-4ae1-efc9-292a952d0469"}, "outputs": [], "source": ["# compute some fitting error\n", "print('MSE = %f eV' % metrics.mean_squared_error(y, y_pred))\n", "print('MAE = %f eV' % metrics.mean_absolute_error(y, y_pred))"]}, {"cell_type": "markdown", "metadata": {"id": "Fk2FfimH592v"}, "source": ["We can also plot the predicted versus the real values to get a visual feel for how well the fitting worked."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 382}, "executionInfo": {"elapsed": 320, "status": "ok", "timestamp": 1646397349521, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "UkASBZVw592v", "outputId": "8307d3f6-d975-4cde-fccf-0b7f43965018"}, "outputs": [], "source": ["plt.figure(dpi=100)\n", "plt.scatter(y, y_pred)\n", "plt.xlabel('Eg True (eV)')\n", "plt.ylabel('Eg Predicted (eV)')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "2CcsnBSL592v"}, "source": ["## Exercise \n", "\n", "By changeing the feature in the $X$ values above try a number of different features. How does it affect the quality of fitting? Report the feature and the MAE and MSE scores in the table below. *Note* to edit the contets of this cell, simply double click on the cell.\n", "\n", "| Feature | MAE (eV) | MSE (eV) |\n", "|---------|----------|----------|\n", "|         |          |          |\n", "|         |          |          |\n", "|         |          |          |\n", "|         |          |          |"]}, {"cell_type": "markdown", "metadata": {"id": "H4EiNNOo592v"}, "source": ["## Multivariate regression\n", "\n", "In a multivariate regression, the input rank $M>1$. Therefore, we will choose a few descriptor to form $x$. Here we choose three descriptors ($M=3$):"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 5, "status": "ok", "timestamp": 1646397349522, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "UUK3fY1P592w"}, "outputs": [], "source": ["# read X\n", "X = oxides[['MagpieData avg_dev CovalentRadius', \n", "            'MagpieData avg_dev Electronegativity', \n", "            'MagpieData maximum NsValence']].values"]}, {"cell_type": "markdown", "metadata": {"id": "9Vv8rjty592w"}, "source": ["And the rest is the same as univariate regression:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# fit linear regression model\n", "model = LinearRegression().fit(X, y)\n", "\n", "# make predictions\n", "y_pred = model.predict(X)\n", "\n", "# compute some fitting error\n", "print('MSE = %f' % metrics.mean_squared_error(y, y_pred))\n", "print('MAE = %f' % metrics.mean_absolute_error(y, y_pred))\n", "\n", "# plot the original and predicted data against each other\n", "plt.figure(dpi=100)\n", "plt.scatter(y, y_pred)\n", "plt.xlabel('Eg True (eV)')\n", "plt.ylabel('Eg Predicted (eV)')\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 417}, "executionInfo": {"elapsed": 563, "status": "ok", "timestamp": 1646397350081, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "i15H0pW3592w", "outputId": "8f47b88f-1bb4-46ac-8e5d-5e89a6824135"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "eQfzJJAK592w"}, "source": ["## Exercise \n", "\n", "By changeing the features in the $X$ values above try a number of different feature combinations. How does it affect the quality of fitting? Report the feature and the MAE and MSE scores in the table below. *Note* to edit the contets of this cell, simply double click on the cell.\n", "\n", "| Feature | MAE (eV) | MSE (eV) |\n", "|---------|----------|----------|\n", "|         |          |          |\n", "|         |          |          |\n", "|         |          |          |\n", "|         |          |          |"]}, {"cell_type": "markdown", "metadata": {"id": "7PT0Sfuy592w"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "jdHzBP0U592x"}, "source": ["# Gradient Boosting Regression\n", "\n", "Gradient boosting is a method for building an ensemble of weak learners to constitute a single strong learner. We build a series of decision trees, each subsequent tree taking in information about the residuals (errors) from the previous trees. In principle, the fitting should improve each time a new tree is added. "]}, {"cell_type": "markdown", "metadata": {"id": "ueuF2NDD592x"}, "source": ["## 1. Create the regressor\n", "\n", "In `sklearn`, a gradient boosting regressor is created by\n", "\n", "```python\n", "GradientBoostingRegressor(loss=<str>, max_depth=<int>, learning_rate=<float>,\n", "                          min_samples_split=<int>, min_samples_leaf=<int>, \n", "                          max_features=<int>, subsample=<float>, n_estimators=<int>)\n", "```\n", "\n", "\n", "The hyperparameters we need to set include:\n", "\n", "* `loss`: a loss function to be minimised. We will use 'lad', which is basically MAE.\n", "* `max_depth`: the maximum depth limits the number of nodes in the trees; its best value depends on the interaction of the input variables; we will start with 10 and can tune it later.\n", "* `learning_rate`: learning rate shrinks the contribution of each tree; there is a trade-off between learning rate and boosting steps; we will start with 0.015 and can tune it later.\n", "* `min_samples_split`: the minimum number of samples required to split an internal node; we will start with 50 and can tune it later.\n", "* `min_samples_leaf`: the minimum number of samples required to be at a leaf node; we set this as 1.\n", "* `max_features`: the number of features to consider when looking for the best split; we will use the number of features in the data.\n", "* `subsample`: the fraction of samples to be used for fitting the individual trees; if smaller than 1.0, this results in Stochastic Gradient Boosting. we will start with 0.9 and can tune it later.\n", "* `n_estimators`: the number of boosting steps or decision trees; we will start with 300 and can tune it later.\n", "\n", "**NOTE**: Simply adding more trees can lead to overfitting. Gradient boosting is quite robust against overfitting, but we will have to look out for this."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 14, "status": "ok", "timestamp": 1646397350082, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "npMeBxQI592x"}, "outputs": [], "source": ["# create the regressor\n", "gbr = GradientBoostingRegressor(loss='lad', max_depth=10, learning_rate=0.015,\n", "                                min_samples_split=50, min_samples_leaf=1, \n", "                                max_features=len(oxides.columns)-1, subsample=0.9, \n", "                                n_estimators=300)"]}, {"cell_type": "markdown", "metadata": {"id": "OEGFYjua592x"}, "source": ["## 2. Fit the regressor\n", "\n", "Here we combine all the descriptors to form $x$ and fit the model:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 8923, "status": "ok", "timestamp": 1646397358995, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "WpMZ4LWO592x", "outputId": "5715622e-b246-43e8-bc62-b77320e4b678"}, "outputs": [], "source": ["# combine all the columns into X\n", "cols = [a for a in list(oxides.columns) if a not in ['gllbsc_gap']]\n", "X = oxides[cols].values\n", "print('Shape of X: %s' % str(X.shape))\n", "\n", "# fit the model\n", "gbr.fit(X, y)"]}, {"cell_type": "markdown", "metadata": {"id": "_1qtH8X-592x"}, "source": ["After fitting the model, we can make predictions and plot them against the original data. The fit has shown a significant improvement over linear regression. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 382}, "executionInfo": {"elapsed": 774, "status": "ok", "timestamp": 1646397485428, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "V-H-GUDC592x", "outputId": "d5d1bd02-9576-49e0-dbd1-c80f6aec5b81"}, "outputs": [], "source": ["# make predictions\n", "y_pred = gbr.predict(X)\n", "\n", "# plot the original and predicted data against each other\n", "plt.figure(dpi=100)\n", "plt.scatter(y, y_pred)\n", "plt.plot([y.min(), y.max()], [y_pred.min(), y_pred.max()],'r')\n", "plt.xlabel('Eg True (eV)')\n", "plt.ylabel('Eg Predicted (eV)')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "AAjPY6gu592x"}, "source": ["## 3. Cross validation\n", "\n", "Cross-validation (CV) allows us to evaluate the out-of-sample goodness-of-fit of the regressor without sparing a validation set. In the basic approach, as called the k-fold CV, the training set is split into $k$ subsets, each serving as the validation set to evaluate the model trained with the other $k-1$ subsets. This approach can be computationally expensive but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage for problems with limited data. Note that a lower CV score means better goodness of fit.\n", "\n", "In the following cell, we compute the scores using 5 folds (so 20% of data for each validation) and the negative MAE as the metric:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 31463, "status": "ok", "timestamp": 1646396255146, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "X24Qnmaj592y", "outputId": "6bf8d980-dd83-4617-977b-28add63e1773"}, "outputs": [], "source": ["# compute cross validation score\n", "scores = cross_val_score(gbr, X, y, cv=5, scoring='neg_mean_absolute_error')\n", "print('Cross validation score: {}'.format(-1 * np.mean(scores)))"]}, {"cell_type": "markdown", "metadata": {"id": "CbXxQ8Nq592y"}, "source": ["## 4. Boosting rate and overfitting\n", "\n", "Let us split the dataset 80:20 into training and test sets. Re-fit the model using the training set only. We can then use some built-in methods of `GradientBoostingRegressor` to get training and test scores at each iteration of boosting. This way, we can check if we have insufficient boosting layers or perhaps we have too many and thus suffer overfitting.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 397}, "executionInfo": {"elapsed": 6598, "status": "ok", "timestamp": 1646396261731, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "A-IrXq5T592y", "outputId": "1b4a7237-fc1b-46a5-ab32-e6c41f443ca5"}, "outputs": [], "source": ["# split the dataset\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n", "\n", "# fit with training set\n", "gbr.fit(X_train, y_train)\n", "\n", "# compute test score at each boosting step\n", "test_score = np.zeros((300,), dtype=np.float64)\n", "for i, y_pred in enumerate(gbr.staged_predict(X_test)):\n", "    test_score[i] = gbr.loss_(y_test, y_pred)\n", "\n", "# plot the scores\n", "plt.figure(dpi=100)\n", "plt.plot(gbr.train_score_, label='Loss on training set')\n", "plt.plot(test_score, label='Loss on test set')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "XDrjS-mK592y"}, "source": ["Notice that the loss of both training and test are still decreasing at 300 steps. We can try to increase the boosting steps to 500 and see if we can still get improvements. If the test score stops increasing, we are probably in a good place to stop extending the model. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 397}, "executionInfo": {"elapsed": 10996, "status": "ok", "timestamp": 1646396272712, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "vFz3tdUS592y", "outputId": "3c2fc7e7-4238-420e-8b60-cabb3381e22b"}, "outputs": [], "source": ["# create the regressor with more boosting steps\n", "gbr500 = GradientBoostingRegressor(loss='lad', max_depth=10, learning_rate=0.015,\n", "                                   min_samples_split=50, min_samples_leaf=1, \n", "                                   max_features=len(oxides.columns)-1, subsample=0.9, \n", "                                   n_estimators=500)\n", "\n", "# fit with training set\n", "gbr500.fit(X_train, y_train)\n", "\n", "# compute test score at each boosting step\n", "test_score = np.zeros((500,), dtype=np.float64)\n", "for i, y_pred in enumerate(gbr500.staged_predict(X_test)):\n", "    test_score[i] = gbr500.loss_(y_test, y_pred)\n", "\n", "# plot the scores\n", "plt.figure(dpi=100)\n", "plt.plot(gbr500.train_score_, label='Loss on training set')\n", "plt.plot(test_score, label='Loss on test set')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "NDpIsXdP592y"}, "source": ["Again, do a 5-fold cross validation at this point. How does the score compare to the earlier one?"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 53784, "status": "ok", "timestamp": 1646396326489, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Txk0aKE_592z", "outputId": "a6ffdf3d-fdb3-4395-86a2-01801084b219"}, "outputs": [], "source": ["# compute cross validation score\n", "scores = cross_val_score(gbr500, X, y, cv=5, scoring='neg_mean_absolute_error')\n", "print('Cross validation score: {}'.format(-1 * np.mean(scores)))"]}, {"cell_type": "markdown", "metadata": {"id": "NEvAOWNg592z"}, "source": ["##  5. Systematic hyperparameter tuning\n", "\n", "Hand tuning a large number of hyperparameters is laborious. Luckily, `sklearn` provides a function [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to automate searches in the hyperparameter space. Even though, performing a grid-search of all of the hyperparameters at once would again lead to a combinatorial explosion. A general strategy for tuning hyperparameters in gradient boosted trees has been suggested [here](https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/). \n", "\n", "1. Choose a relatively high learning rate. Generally the default value of 0.1 works but somewhere between 0.05 to 0.2 should work for different problems.\n", "2. Determine the optimum number of trees for this learning rate. This should range around 40 to 90. Remember to choose a value on which your system can work fairly fast. This is because it will be used for testing various scenarios and determining the tree parameters.\n", "3. Tune tree-specific parameters for decided learning rate and number of trees. \n", "4. Lower the learning rate and increase the estimators proportionally to get more robust models.\n", "\n", "We will follow the above process to tune our regressor.\n", "\n", "\n", "### Step 1 & 2: Optimise `n_estimators` with `learning_rate=0.1`"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 33992, "status": "ok", "timestamp": 1646396360467, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "U79suuqH592z", "outputId": "2e65be89-2f13-48c8-b65b-f3ec7038d2e9"}, "outputs": [], "source": ["# candidates\n", "param_test_n_est = {'n_estimators': range(40, 90, 10)}\n", "\n", "# create the regressor\n", "gbr_n_est = GradientBoostingRegressor(loss='lad', learning_rate=0.1, \n", "                                      max_features=len(cols), max_depth=10,\n", "                                      min_samples_split=50, subsample=0.9,\n", "                                      random_state=0)\n", "\n", "# define hyperparameter search\n", "gsearch = GridSearchCV(estimator= gbr_n_est, param_grid = param_test_n_est, \n", "                       scoring='neg_median_absolute_error', cv=5)\n", "\n", "# perform search\n", "gsearch.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 17, "status": "ok", "timestamp": 1646396360468, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Se1JjZ33592z", "outputId": "e40e7fc7-5311-462e-8fbf-81b0334ae45b"}, "outputs": [], "source": ["# print best n_estimators\n", "gsearch.best_params_"]}, {"cell_type": "markdown", "metadata": {"id": "zm24pfjC592z"}, "source": ["### Step 3: Optimise tree parameters with best `n_estimators`\n", "\n", "Here we consider `max_depth` and `min_samples_split`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 226432, "status": "ok", "timestamp": 1646396586891, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "U-08ufTM592z", "outputId": "e3ea6a09-8fe8-46ae-d8ad-7fbd233bb825"}, "outputs": [], "source": ["# candidates\n", "param_test_tree = {'max_depth': range(5, 16, 2), \n", "                   'min_samples_split': range(10, 100, 20)}\n", "\n", "# create the regressor\n", "gbr_tree = GradientBoostingRegressor(loss='lad', learning_rate=0.1, \n", "                                     max_features=len(cols), subsample=0.9,\n", "                                     n_estimators=70, random_state=0)\n", "\n", "# define hyperparameter search\n", "gsearch = GridSearchCV(estimator= gbr_tree, param_grid = param_test_tree, \n", "                       scoring='neg_median_absolute_error', cv=5)\n", "\n", "# perform search\n", "gsearch.fit(X, y)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 13, "status": "ok", "timestamp": 1646396586892, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "F6C2T5W-5920", "outputId": "6dd8b37e-1e89-4c02-d3c8-8dd0bc085b55"}, "outputs": [], "source": ["# print best max_depth and min_samples_split\n", "gsearch.best_params_"]}, {"cell_type": "markdown", "metadata": {"id": "6C74EE_w5920"}, "source": ["### Step 4: Lower `learning_rate` and increase `n_estimators`\n", "\n", "Here we use a factor of 5, so `learning_rate` is lowered to 0.02 and `n_estimators` increased to 350:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 9542, "status": "ok", "timestamp": 1646396596429, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "bUm71vSN5920", "outputId": "9ee95df1-9f2b-4307-a569-ab728c4cbf5c"}, "outputs": [], "source": ["# create the \"optimised\" regressor\n", "gbr_opt = GradientBoostingRegressor(loss='lad', learning_rate=0.02, \n", "                                    max_features=len(cols), max_depth=7,\n", "                                    min_samples_split=10, subsample=0.9,\n", "                                    n_estimators=350, random_state=0)\n", "\n", "# fit the model\n", "gbr_opt.fit(X, y)"]}, {"cell_type": "markdown", "metadata": {"id": "uPioev1y5920"}, "source": ["Eventually, we can use our \"optimised\" model to make predictions and compute CV scores:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 553}, "executionInfo": {"elapsed": 38992, "status": "ok", "timestamp": 1646396635406, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "ttglg7w65920", "outputId": "c9ba1d58-ad06-43e7-a41d-500fc0eae5e1"}, "outputs": [], "source": ["# make predictions\n", "y_pred = gbr_opt.predict(X)\n", "\n", "# plot the original and predicted data against each other\n", "plt.figure(dpi=100)\n", "plt.scatter(y, y_pred)\n", "plt.show()\n", "\n", "# compute cross validation score\n", "scores = cross_val_score(gbr_opt, X, y, cv=5, scoring='neg_mean_absolute_error')\n", "print('Cross validation score: {}'.format(-1 * np.mean(scores)))"]}, {"cell_type": "markdown", "metadata": {"id": "Eut-UUJo5920"}, "source": ["**Yes, our efforts pay off**, as shown by the figure and the CV score!"]}, {"cell_type": "markdown", "metadata": {"id": "sOa8jgEC5921"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "K3bt_8MT5921"}, "source": ["## Exercises \n", "\n", "Similar to [classification_decision_tree.ipynb](classification_decision_tree.ipynb), use regression trees to fit one or some of the standard \"toy\" datasets embedded in `sklearn`, such as `boston-house-prices` and `diabetes`. These datasets are less challenging than our example."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 19, "status": "ok", "timestamp": 1646396635407, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "6Vk_g26c5921", "outputId": "f11f321e-c66c-4a70-9f8b-4ca7e1f7d095"}, "outputs": [], "source": ["# load iris dataset\n", "boston = sklearn.datasets.load_boston()\n", "print(boston['DESCR'])"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "mQoFmYa75922"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "Pp7KahJh5922"}, "outputs": [], "source": []}], "metadata": {"colab": {"collapsed_sections": [], "name": "Material_Oxides regression_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}