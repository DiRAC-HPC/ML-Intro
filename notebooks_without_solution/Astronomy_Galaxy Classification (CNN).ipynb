{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 0. Galaxy classification using convolutional neural networks\n", "\n", "In this notebook, we attempt to classify galaxies according to their shape using CNN.\n", "\n", "\n", "The data in this example is a subset of the [Galaxy10 dataset](https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/). The images are categorized according to their shapes. We use a slimmed down dataset which contains 8 classes:\n", "\n", "'Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral', 'Unbarred Tight Spiral', 'Unbarred Loose Spiral', 'Edge-on without Bulge', 'Edge-on with Bulge'.\n", "\n", "\n", "The aim of this work is to train a neural network to classify the galaxy images. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import h5py\n", "from sklearn.model_selection import train_test_split\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "The slimmed down dataset, which include the images with 8 labels, are stored in the h5 file `Galaxy8.h5`. We load this file using a `h5py` library.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# load dataset\n", "with h5py.File(join(data_path, 'Astronomy/Galaxy8.h5'), 'r') as F:\n", "    x = np.array(F['images'])\n", "    y = np.array(F['ans'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# downsample the images - so that the model fits in memory.\n", "x = x[:, ::2, ::2]\n", "\n", "# normalise images    \n", "x = x / 255.0\n", "\n", "# split train test\n", "train_images, test_images, train_labels, test_labels = train_test_split(x, y, test_size=0.2, random_state=0)\n", "\n", "\n", "# string labels\n", "string_labels = ['Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral',\n", "                 'Unbarred Tight Spiral', 'Unbarred Loose Spiral', 'Edge-on without Bulge', 'Edge-on with Bulge']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0, :, :, 0].shape))\n", "print(\"Number of channels: %s\" % str(train_images.shape[-1]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We can randomly plot some images and their labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image.squeeze(), cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=8)\n", "    plt.title(label2, c=label2_color, fontsize=8, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Build the network\n", "\n", "In this case, we have 8 classes. Hence, the number of the outputs is 8. Then, we use 'SparseCategoricalCrossentropy' and will see 'accuracy' while training the model.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the network architecture\n", "model = Sequential()\n", "model.add(Conv2D(16, (5, 5), activation='relu', input_shape=(64, 64, 3)))\n", "model.add(MaxPool2D((2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Conv2D(32, (3, 3), activation='relu'))\n", "model.add(MaxPool2D((2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Conv2D(64, (3, 3), activation='relu'))\n", "model.add(MaxPool2D((2, 2)))\n", "model.add(BatchNormalization())\n", "model.add(Flatten())\n", "model.add(Dense(128, activation='relu'))\n", "model.add(Dropout(0.4))\n", "model.add(Dense(8, activation='sigmoid'))\n", "\n", "# print summary\n", "model.summary()\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Compile and train the model\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# optimizer, loss, metrics\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])\n", "\n", "# train the model\n", "training_history = model.fit(train_images, train_labels, epochs=50, batch_size=32, \n", "                             validation_data=(test_images, test_labels))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results \n", "\n", "### Check training history\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n", "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(training_history.history['loss'], label='Loss on training data')\n", "plt.plot(training_history.history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# use test images to predict\n", "pred_lables = model.predict(test_images).argmax(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# get the indices of wrong predictions\n", "id_wrong = np.where(pred_lables != test_labels)[0]\n", "print(\"Number of test data: %d\" % test_labels.size)\n", "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n", "\n", "# plot the wrong predictions\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n", "    label = \"%d: %s\" % (test_labels[idata], string_labels[test_labels[idata]])\n", "    label2 = \"%d: %s\" % (pred_lables[idata], string_labels[pred_lables[idata]])\n", "    subplot_image(test_images[idata], label, nrows, ncols, iplot, label2, 'r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Exercises\n", "\n", "* Classify a more complicated dataset, e.g., a randomly rotated galaxies as shown below. For the augmented dataset, compare the accuracy of a CNN and a fully connected DNN.  "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# rotate images randomly by k * 90 degrees\n", "# train\n", "train_images_rot = train_images.copy()\n", "for i in np.arange(train_images.shape[0]):\n", "    train_images_rot[i] = np.rot90(train_images[i], k=np.random.choice([0, 1, 2, 3]))\n", "    \n", "# test\n", "test_images_rot = test_images.copy()\n", "for i in np.arange(test_images.shape[0]):\n", "    test_images_rot[i] = np.rot90(test_images[i], k=np.random.choice([0, 1, 2, 3]))\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images_rot[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 4}