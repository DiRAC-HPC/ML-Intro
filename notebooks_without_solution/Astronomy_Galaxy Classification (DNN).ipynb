{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 0. Galaxy classification using fully-connected neural networks\n", "\n", "In this notebook, we attempt to classify galaxies according to their shape using fully connected DNN.\n", "\n", "\n", "The data in this example is a subset of the [Galaxy10 dataset](https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/). The images are categorized according to their shapes. We use a slimmed down dataset which contains 8 classes:\n", "\n", "'Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral', 'Unbarred Tight Spiral', 'Unbarred Loose Spiral', 'Edge-on without Bulge', 'Edge-on with Bulge'.\n", "\n", "\n", "The aim of this work is to train a neural network to classify the galaxy images. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Flatten, Dropout\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import h5py\n", "from sklearn.model_selection import train_test_split\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "The slimmed down dataset, which include the images with 8 labels, are stored in the h5 file `Astronomy/Galaxy8.h5`. We load this file using a `h5py` library.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# load dataset\n", "with h5py.File(join(data_path, 'Astronomy/Galaxy8.h5'), 'r') as F:\n", "    x = np.array(F['images'])\n", "    y = np.array(F['ans'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# downsample the images - so that the model fits in memory.\n", "x = x[:, ::2, ::2]\n", "\n", "# normalise images    \n", "x = x / 255.0\n", "\n", "# split train test\n", "train_images, test_images, train_labels, test_labels = train_test_split(x, y, test_size=0.2, random_state=0)\n", "\n", "\n", "# string labels\n", "string_labels = ['Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral',\n", "                 'Unbarred Tight Spiral', 'Unbarred Loose Spiral', 'Edge-on without Bulge', 'Edge-on with Bulge']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n", "print(\"Image pixels: %s\" % str(train_images[0, :, :, 0].shape))\n", "print(\"Number of channels: %s\" % str(train_images.shape[-1]))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We can randomly plot some images and their labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image, cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=8)\n", "    plt.title(label2, c=label2_color, fontsize=8, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Build the network\n", "\n", "In this case, we have 8 classes. Hence, the number of the outputs is 8. Then, we use 'SparseCategoricalCrossentropy' and will see 'accuracy' while training the model.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the network architecture\n", "model = Sequential()\n", "model.add(Flatten(input_shape=(64, 64, 3)))\n", "model.add(Dense(1024, activation='relu'))\n", "model.add(Dense(128, activation='relu'))\n", "model.add(Dense(8, activation='sigmoid'))\n", "\n", "# print summary\n", "model.summary()\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Compile and train the model\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# compile the model\n", "model.compile(optimizer='adam',\n", "              loss=keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])\n", "\n", "# train the model\n", "training_history = model.fit(train_images, train_labels, epochs=50, batch_size=32, \n", "                             validation_data=(test_images, test_labels))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Check training history\n", "\n", "We can examine the training history by plotting accuracy and loss against epoch for both the training and the test data. \n", "\n", "Notice that the accuracies for the training and the test data diverge as the model trains. This is a classic symptom of [overfitting](https://en.wikipedia.org/wiki/Overfitting), that is, our model corresponds too closely to the training data so that it cannot fit the test data with an equivalent accuracy.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n", "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(training_history.history['loss'], label='Loss on training data')\n", "plt.plot(training_history.history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Regularise and re-train\n", "\n", "Dropout, also called dilution, is a regularisation technique to mitigate against overfitting by randomly omitting a certain amount of neurons from a layer. Here we will rebuild our model with `Dropout` between the hidden and the output layers. Let us see whether this can negate the overfitting or not.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the network architecture\n", "model_reg = Sequential()\n", "model_reg.add(Flatten(input_shape=(64, 64, 3)))\n", "model_reg.add(Dense(1024, activation='relu'))\n", "model_reg.add(Dropout(0.1))\n", "model_reg.add(Dense(128, activation='relu'))\n", "model_reg.add(Dense(8, activation='sigmoid'))\n", "\n", "# compile the model\n", "model_reg.compile(optimizer='adam',\n", "                  loss=keras.losses.SparseCategoricalCrossentropy(),\n", "                  metrics=['accuracy'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the model\n", "training_history_reg = model_reg.fit(train_images, train_labels, epochs=50, batch_size=32, \n", "                                     validation_data=(test_images, test_labels))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Compare training histories with and without dropout\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(training_history_reg.history[acc_str], label='Accuracy on training data')\n", "plt.plot(training_history_reg.history['val_' + acc_str], label='Accuracy on test data')\n", "plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data (no dropout)')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(training_history_reg.history['loss'], label='Loss on training data')\n", "plt.plot(training_history_reg.history['val_loss'], label='Loss on test data')\n", "plt.plot(training_history.history['val_loss'], label='Loss on test data (no dropout)')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Make predictions\n", "\n", "Finally, we can use our trained model to make predictions. Here we show some wrong predictions for the test data, from which we may get some ideas about what kinds of images baffle our model."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# use test images to make predictions\n", "pred_lables = model_reg.predict(test_images).argmax(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# get the indices of wrong predictions\n", "id_wrong = np.where(pred_lables != test_labels)[0]\n", "print(\"Number of test data: %d\" % test_labels.size)\n", "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n", "\n", "# plot the wrong predictions\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n", "    label = \"%d: %s\" % (test_labels[idata], string_labels[test_labels[idata]])\n", "    label2 = \"%d: %s\" % (pred_lables[idata], string_labels[pred_lables[idata]])\n", "    subplot_image(test_images[idata], label, nrows, ncols, iplot, label2, 'r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Exercises\n", "\n", "* Change some hyperparameters in `model.compile()` and `model.fit()` to see their effects (see reference of [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)); \n", "* Regularise the network and re-train"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 2}