{"cells": [{"cell_type": "markdown", "metadata": {"id": "uPGPddfzXUpE"}, "source": ["# 0. Ag Detection by Muon Spectroscopy\n", "\n", "In this notebook, we attempt to solve a real problem in physics using a fully connected DNN.\n", "\n", "\n", "The data in this example is generated from simulated muon spectroscopy experiments. First the data was generated for each individual element by simulating the spectral emission lines of that element. Then for the mixed compounds the different elemental spectra were mixed in proportion to how much of that element is present in the compound. \n", "\n", "\n", "The aim of this work is to train a neural network to detect the presence of Ag. "]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 3536, "status": "ok", "timestamp": 1646394631798, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "7qxVsjoiXUpI", "outputId": "0f1b566f-c6a8-4ea6-cebe-6c1ffb98fc34"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Dense, Flatten, Dropout, BatchNormalization\n", "from tensorflow.keras.optimizers import Adam\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import time\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)"]}, {"cell_type": "markdown", "metadata": {"id": "atz7q0VFfDd2"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "W_l1FqxjZCUj"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 18322, "status": "ok", "timestamp": 1646394666401, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "69j2tl_HZNCJ", "outputId": "dfef10c8-c0fa-4c7e-b2ad-ea0c5999ed87"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "zZYqAI_jXUpK"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "5GwuX5eoXUpK"}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "The raw data, which include the constituent elements and the Muon spectra of the samples, are stored in the pickle file `muon-data/ag-muon-data-tight.pkl`. We load this file into a `pandas` dataframe and take a quick look."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 241}, "executionInfo": {"elapsed": 25722, "status": "ok", "timestamp": 1646394692119, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "0G47adNgXUpK", "outputId": "805fa637-34f2-49fd-c08a-b778fb77ed82"}, "outputs": [], "source": ["# read data\n", "df = pd.read_pickle(join(data_path, 'muon-data/ag-muon-data-tight.pkl')) \n", "#print dimensions\n", "print('Number of samples in the dataset: %d' % len(df['Spectra']))\n", "print('Length of spectra for each sample: %d' % len(df['Spectra'][0]))\n", "\n", "# print the first few data\n", "df.head(n=5)"]}, {"cell_type": "markdown", "metadata": {"id": "V22bV9g5XUpL"}, "source": ["In the above table, the `Elements` and the `Spectra` columns show respectively the elements and the spectra of the samples. There are 138,613 samples in the dataset, and each spectrum is a series of 1000 positive reals. \n", "\n", "To get a feel for the complexity of picking out signals with Ag in multinary samples, we can plot some random spectra for three representative cases: \n", "\n", "* no Ag\n", "* pure Ag\n", "* Ag-Si binary\n", "\n", "Note that we are plotting only the first part of each spectrum. Change `[0:150]` to `[:]` to show the full spectra."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 970}, "executionInfo": {"elapsed": 4257, "status": "ok", "timestamp": 1646394696370, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "O8iEaw9XXUpL", "outputId": "ff0d530b-e09e-44fd-8a23-14abbd175c50"}, "outputs": [], "source": ["# conditions to select data\n", "conditions = [\n", "# no Ag\n", "('no Ag', np.where(['Ag' not in elements for elements in df['Elements']])[0]),\n", "# pure Ag\n", "('pure Ag', np.where([['Ag'] == elements for elements in df['Elements']])[0]),\n", "# Ag-Si\n", "('Ag-Si binary', np.where([['Ag', 'Si'] == elements for elements in df['Elements']])[0])\n", "]\n", "\n", "# plot\n", "ncond = len(conditions)\n", "nplot = 4 # number of plots per condition\n", "fig, axs = plt.subplots(nplot, ncond, dpi=200, figsize=(ncond * 5, nplot * 2), sharex=True, sharey=True)\n", "plt.subplots_adjust(wspace=.1, hspace=.2)\n", "for icond, cond in enumerate(conditions):\n", "    for iplot, idata in enumerate(np.random.choice(cond[1], nplot)):\n", "        axs[iplot, icond].plot(df['Spectra'][idata][150:700], c='C%d' % icond)\n", "        axs[iplot, icond].set_xlabel('Sample %d: %s' % (idata, cond[0]), c='C%d' % icond)\n", "        axs[iplot, icond].set_ylim(0, 100)"]}, {"cell_type": "markdown", "metadata": {"id": "QkhxXy4jXUpM"}, "source": ["### Prepare training data\n", "\n", "The input data for our network will be the `Spectra` column, and we can use the `to_list()` method to convert it to a numpy array. The output data for our network will be a binary-valued one-hot vector: 0 for no Ag in the sample and 1 otherwise. One-hot encoding can be achieved by a simple for-loop. Also, it is important to normalise each spectrum between 0 and 1."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 373, "status": "ok", "timestamp": 1646394696738, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "BoZ4VKKYXUpM", "outputId": "41ede01d-d0e8-49a2-e2d1-47f1296e7d94"}, "outputs": [], "source": ["###### input ######\n", "# convert the 'Spectra' column to numpy\n", "x_train = np.array(df['Spectra'].to_list())\n", "# normalise each spectrum to [0, 1]\n", "x_train /= np.max(x_train, axis=1)[:, np.newaxis]\n", "\n", "###### output ######\n", "# one-hot encoding: whether Ag is in 'Elements'\n", "y_train = np.array(['Ag' in elements for elements in df['Elements']]).astype(int)\n", "\n", "# print data shapes\n", "print(\"Shape of input: %s\" % str(x_train.shape))\n", "print(\"Shape of output: %s\" % str(y_train.shape))"]}, {"cell_type": "markdown", "metadata": {"id": "Wo5Qy2vVXUpN"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "VBxA3ZUKXUpN"}, "source": ["# 2. Build the network\n", "\n", "In this case, the output is whether the data includes 'Ag' (1) or not (0). Therefore, the size of the outout of the last dense layer is 1. Then, we use 'binary_Crossentropy' and will see 'accuracy' while training the model.\n", "\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# define the model\n", "model = Sequential()\n", "model.add(Dense(64, input_dim=1000, activation='relu'))\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "# print model summary\n", "model.summary()\n", "\n", "# compile the model\n", "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 18, "status": "ok", "timestamp": 1646394696740, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "LKyTW_jQXUpN", "outputId": "bd056de3-3378-49ee-f61f-7167d59d534f"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "tE4PKAr4XUpO"}, "source": ["### Train the model\n", "\n", "Since we have not separated a subset of data for validation, we can pass `validation_split=0.2` to `model.fit()`, which then will use the *final* 20% of the dataset for validation. Let us do 10 epochs first.\n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# train the model\n", "\n", "training_history = model.fit(x_train, y_train, epochs=10, batch_size=64, \n", "                             validation_split=0.2)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 82425, "status": "ok", "timestamp": 1646394779159, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "-RF3ylRvuzEI", "outputId": "9d521ce3-f677-43a2-adcb-763968caf57f"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "vip5G3R1ha8c"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "_-4zVVqGhcCZ"}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {"id": "vwGR_9cgXUpO"}, "source": ["### Plot training history\n", "\n", "For convenience, we define a function to plot a training history:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "f-r-EBsOXUpO"}, "outputs": [], "source": ["# a function to plot training history\n", "def plot_history(training_history):\n", "    # plot accuracy\n", "    plt.figure(dpi=100, figsize=(12, 4))\n", "    plt.subplot(1, 2, 1)\n", "    plt.plot(training_history.history[acc_str], label='Accuracy on training data')\n", "    plt.plot(training_history.history['val_' + acc_str], label='Accuracy on test data')\n", "    plt.legend()\n", "    plt.title(\"Accuracy\")\n", "\n", "    # plot loss\n", "    plt.subplot(1, 2, 2)\n", "    plt.plot(training_history.history['loss'], label='Loss on training data')\n", "    plt.plot(training_history.history['val_loss'], label='Loss on test data')\n", "    plt.legend()\n", "    plt.title(\"Loss\")\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "Hv-rDboAXUpO"}, "source": ["Now, plot the training history of the current model. They will look bizarre at this stage, as explained in the forthcoming section.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot training history\n", "plot_history(training_history)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 385}, "executionInfo": {"elapsed": 674, "status": "ok", "timestamp": 1646394779830, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "g4mYmzBKXUpO", "outputId": "98d0141b-abb3-4d27-90c3-8661ca0dbff2"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "7_Mer9nvhkNz"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "SGFur17qhlIC"}, "source": ["# 4. Exercise"]}, {"cell_type": "markdown", "metadata": {"id": "rOX8fPbwXUpP"}, "source": ["## Class imbalance\n", "\n", "In the above history plot, notice how the accuracy of the model converges to a high value very quickly (>90% at the end of the first epoch). Such an odd history indicates that something could be wrong within our dataset."]}, {"cell_type": "markdown", "metadata": {"id": "Ook9v_A_XUpP"}, "source": ["### Data distribution\n", "\n", "Let us inspect the distribution of the data using `plt.hist(y_train)`, paying special attention to the validation part (the final 20%)."]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 382}, "executionInfo": {"elapsed": 10, "status": "ok", "timestamp": 1646394779831, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "3FAN81owXUpP", "outputId": "04fc88cf-4736-450a-bed2-8673a33f81f5"}, "outputs": [], "source": ["# plot distribution of data\n", "plt.figure(dpi=100)\n", "plt.hist(y_train, label='Whole dataset')\n", "plt.hist(y_train[-len(y_train)//5:], label='Validation subset')\n", "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n", "plt.xlabel('label')\n", "plt.ylabel('number of data')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "jBizmwFCXUpP"}, "source": ["The histograms show that our dataset is dominated by samples labelled 0 or \"no Ag\", which account for over 95% of the data. Thus, if the model simply learns to *guess* \"no Ag\" in every sample, it can achieve 95% accuracy without learning anything meaningful. This problem is known as **class imbalance**.\n", "\n", "To avoid this, we must balance the classes. There are a number of strategies we can take:\n", "\n", "* Upsample the minority class;\n", "* Downsample the majority class;\n", "* Change the performance metric.\n", "\n", "The best available option for our problem is to downsample the majority class, which can be easily achieved with `numpy`:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "z4C7tJQRXUpP"}, "outputs": [], "source": ["# find original indices of 0 ('no Ag') and 1 ('with Ag')\n", "id_no_Ag = np.where(y_train == 0)[0]\n", "id_with_Ag = np.where(y_train == 1)[0]\n", "\n", "# downsample 'no Ag' to the number of 'with Ag' by np.random.choice\n", "id_no_Ag_downsample = np.random.choice(id_no_Ag, len(id_with_Ag))\n", "\n", "# concatenate 'with Ag' and downsampled 'no Ag'\n", "id_downsample = np.concatenate((id_with_Ag, id_no_Ag_downsample))\n", "\n", "# shuffle the indices because they are ordered after concatenation\n", "np.random.shuffle(id_downsample)\n", "\n", "# finally get the balanced data\n", "x_train_balanced = x_train[id_downsample]\n", "y_train_balanced = y_train[id_downsample]"]}, {"cell_type": "markdown", "metadata": {"id": "M32U3f24XUpP"}, "source": ["Re-exam the histograms of the balanced dataset after downsampling the majority:"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 382}, "executionInfo": {"elapsed": 422, "status": "ok", "timestamp": 1646394780245, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Hk6qVarBXUpQ", "outputId": "dcfb7100-bd15-4b20-8a86-9fb531951425"}, "outputs": [], "source": ["# plot distribution of downsampled data\n", "plt.figure(dpi=100)\n", "plt.hist(y_train_balanced, label='Whole dataset')\n", "plt.hist(y_train_balanced[-len(y_train_balanced)//5:], label='Validation subset')\n", "plt.xticks([0, 1], ['0: no Ag', '1: with Ag'])\n", "plt.xlabel('label')\n", "plt.ylabel('number of data')\n", "plt.legend()\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "koo4DUqNXUpQ"}, "source": ["### Re-train the model\n", "\n", "Now we can re-train the model with the balanced dataset. Simply change `x_train` and `y_train` to `x_train_balanced` and `y_train_balanced` in `model.fit()` and repeat all the steps in [1. Try out a network](#1.-Try-out-a-network). Note that, to avoid the influence of the initial model state (weights and biases) left by the previous training (such as the one trained with the imbalanced dataset), we have to first re-define and re-compile the model before calling `model.fit()`. A larger `epochs` can be used because we now have much fewer data. \n", "\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# set a random seed\n", "tf.random.set_seed(0)\n", "\n", "# re-define the model\n", "model = Sequential()\n", "model.add(Dense(64, input_dim=1000, activation='relu'))\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "# re-compile the model\n", "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "\n", "# re-train the model\n", "tic = time.time()\n", "training_history_balanced = model.fit(x_train_balanced, y_train_balanced, \n", "                                      epochs=500, batch_size=256, validation_split=0.2)\n", "print('run time: {:5.3f} s'.format(time.time() - tic))\n", "\n", "# plot training history\n", "plot_history(training_history_balanced)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"elapsed": 201532, "status": "ok", "timestamp": 1646394981769, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "zIdxXMQHXUpQ", "outputId": "1d3cf4a5-d39b-423a-85a9-6ca857525f24"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 385}, "executionInfo": {"elapsed": 749, "status": "ok", "timestamp": 1646394982513, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "Zl8Efd_eXUpQ", "outputId": "f0d793fb-97f9-46fa-f88d-3e3f49f18047"}, "outputs": [], "source": ["# plot training history\n", "plot_history(training_history_balanced)"]}, {"cell_type": "markdown", "metadata": {"id": "2JrVwj2GXUpQ"}, "source": ["### Early stopping\n", "\n", "The droput has brought the training and validation losses closer to one another. However we also see that there are large oscillations in the validation performance. This is realted to the rather small training and validation set sizes. How can we make sure that we recover the model with the best performance on validation?\n", "\n", "We can use a **callback** to tell the algorithm to save the model every time there is a new best validation loss. We can then also tell the training algorithm to cease if the validation loss has not improved for _n_ steps; below we set up a callback to monitor the `val_accuracy` and to stop if this has not improved for 20 steps, then restore the weights of the best model. \n", "\n", "```\n", "callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n", "```\n", "\n", "The callbacks are then specified as a list of all callbacks you defined and passed to the `fit` function _via_ the keyword `callbacks`.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# set a random seed\n", "tf.random.set_seed(0)\n", "\n", "# re-define the model\n", "model = Sequential()\n", "model.add(Dense(64, input_dim=1000, activation='relu'))\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(1, activation='sigmoid'))\n", "\n", "# re-compile the model\n", "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "\n", "# define the callback\n", "callback = EarlyStopping(monitor='loss', patience=20, restore_best_weights=True)\n", "\n", "# re-train the model\n", "tic = time.time()\n", "training_history_balanced = model.fit(x_train_balanced, y_train_balanced, \n", "                                      epochs=500, batch_size=256, validation_split=0.2,\n", "                                      callbacks=[callback])\n", "print('run time: {:5.3f} s'.format(time.time() - tic))\n", "    \n", "# plot training history\n", "plot_history(training_history_balanced)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 1000}, "executionInfo": {"elapsed": 110308, "status": "ok", "timestamp": 1646395092813, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "45R8ltS7XUpQ", "outputId": "62f81871-a035-4dc5-a66a-b6f5810199d6"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "MV-CKnZxXUpQ"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "w6Ip5V54XUpQ"}, "source": ["## More exercises\n", "\n", "Build a DNN to detect the presence of all the elements. To do this, you may go through the following steps:\n", "\n", "1. Find all the elements appearing in the dataset; the answer will be `['Zn', 'Sb', 'Si', 'Fe', 'Ag', 'Cu', 'Bi']`.\n", "2. Balance the dataset: if one of the elements appears much less times than the others, it is better to ignore it. Doing everything correctly, you will find the number of samples containing each element as shown in the following table. Therefore, we may ignore Ag in this network.\n", "\n", "\n", "|  Element | # Samples |\n", "|---|---|\n", "|Zn| 51174|  \n", "|Sb| 51132|  \n", "|Si| 50909| \n", "|Fe| 50764|\n", "|Ag| 10000|\n", "|Cu| 50945|\n", "|Bi| 50784|\n", "    \n", "3. Do one-hot encoding for the element list `['Zn', 'Sb', 'Si', 'Fe', 'Cu', 'Bi']`; if a sample contains Fe and Sb, e.g., the one-hot vector for this sample will be `[0, 1, 0, 1, 0, 0]`.\n", "4. Build and train a DNN (with an input size of 150 and an output size of 6) to detect the presence of the six elements.\n", "\n", "If doing everything correctly, you will find that the overall accuracy is around 60%. However, the model is not garbage. If we evaluate the accuracy for each element, we will find that the accracy for some of elements is nearly 0 while for the others nearly 100%. This means the dataset is agnostic to these elements, which lower the overall accuracy, but the model can still be used to predict the other elements with  high accuracy.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "##################\n", "###### data ######\n", "##################\n", "# output\n", "element_list = ['Zn', 'Sb', 'Si', 'Fe', 'Cu', 'Bi']\n", "y_train = []\n", "for element in element_list:\n", "    y_train.append(np.array([element in elements for elements in df['Elements']]).astype(int))\n", "y_train = np.transpose(np.array(y_train))\n", "\n", "# print data shapes\n", "print(\"Shape of input: %s\" % str(x_train.shape))\n", "print(\"Shape of output: %s\" % str(y_train.shape))\n", "\n", "\n", "###################\n", "###### model ######\n", "###################\n", "model = Sequential()\n", "model.add(Dense(64, input_dim=1000, activation='relu'))\n", "model.add(Dense(16, activation='relu'))\n", "model.add(Dense(6, activation='sigmoid'))\n", "\n", "# print model summary\n", "model.summary()\n", "\n", "# compile the model\n", "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n", "\n", "\n", "###################\n", "###### train ######\n", "###################\n", "# train the model\n", "training_history = model.fit(x_train, y_train, epochs=100, batch_size=256, \n", "                             validation_split=0.2)\n", "# plot training history\n", "plot_history(training_history)\n", "\n", "\n", "#####################\n", "###### predict ######\n", "#####################\n", "y_pred = model.predict(x_train)\n", "\n", "# show overall accuracy and accuracy for each element\n", "def binary_accuracy(pred, ground_truth):\n", "    return np.where(np.abs(pred - ground_truth) < .1)[0].size / pred.size\n", "\n", "print(f'Overall accuracy = {binary_accuracy(y_pred, y_train)}')\n", "for i, element in enumerate(element_list):\n", "    print(f'Accuracy for {element} = {binary_accuracy(y_pred[:, i], y_train[:, i])}')\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "7BuyXKJxXUpR"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "7Vkp49DxXUpR"}, "outputs": [], "source": []}], "metadata": {"colab": {"collapsed_sections": [], "name": "Material_Ag Detection by Muon Spectroscopy_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}