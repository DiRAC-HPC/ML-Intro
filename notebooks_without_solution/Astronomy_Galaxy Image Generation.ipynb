{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 0. Galaxy image generation using VAEs.\n", "\n", "In this notebook, we attempt to generate new galaxy images using Variational autoencoders.\n", "\n", "Variation autoencoders (VAEs) are an extended type of autoencoders (AEs). A VAE can enhance the robustness of content generation by regularising the encodings distribution in the latent space. In this notebook, we will go through the fundamentals of VAEs (motivation, theory and Keras-based implementation) using a subset of the [Galaxy10 dataset](https://www.zooniverse.org/projects/zookeeper/galaxy-zoo/). We use a slimmed down dataset which contains 5 classes:\n", "\n", "'Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral', 'Edge-on without Bulge'.\n", "\n", "The aim of this work is to train a neural network to generate new galaxy images and learn their latent representations. \n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras import layers\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import h5py\n", "from sklearn.model_selection import train_test_split\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "The slimmed down dataset, which include the images with 8 labels, are stored in the h5 file `Astronomy/Galaxy5.h5`. We load this file using a `h5py` library.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# load dataset\n", "with h5py.File(join(data_path, 'Astronomy/Galaxy5.h5'), 'r') as F:\n", "    x = np.array(F['images'])\n", "    y = np.array(F['ans'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# downsample the images - so that the model fits in memory.\n", "x = x[:, ::2, ::2]\n", "\n", "# normalise images    \n", "x = x / 255.0\n", "\n", "# split train test\n", "train_images, test_images, train_labels, test_labels = train_test_split(x, y, test_size=0.2, random_state=0)\n", "\n", "\n", "# string labels\n", "string_labels = ['Disturbed', 'Merging', 'Round Smooth', 'Barred Spiral', 'Edge-on without Bulge']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(train_labels))\n", "print(\"Number of test data: %d\" % len(test_labels))\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### We can randomly plot some images and their labels"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image, cmap=plt.cm.binary)\n", "    plt.xlabel(label, c='k', fontsize=8)\n", "    plt.title(label2, c=label2_color, fontsize=8, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(train_labels), nrows * ncols)):\n", "    label = \"%d: %s\" % (train_labels[idata], string_labels[train_labels[idata]])\n", "    subplot_image(train_images[idata].reshape(64, 64, 3), label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2.Autoencoders and Regularity of Latent Space"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Why we need a VAE? To answer this question, let us start with an ordinary AE and see what is unsatisfactory when we use it to generate new images. \n", "\n", "## Build and train an autoencoder\n", "\n", "We can quickly build an AE with `Dense` layers. First, we specify the latent dimension or the size of the bottleneck; we use 2.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# latent dimension\n", "latent_dim = 2\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### The encoder\n", "\n", "The encoder contains four layers, an input layer with size 128$\\times$128, two hidden layers with sizes 2048, 512 and 128, respectively, and the latent output layer:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the encoder\n", "image_input = keras.Input(shape=(64, 64, 3))\n", "x = layers.Conv2D(32, 4, 4, padding = 'same')(image_input)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2D(64, 4, 4, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Flatten()(x)\n", "x = layers.Dense(64, activation=\"relu\")(x)\n", "latent_output = layers.Dense(latent_dim)(x)\n", "encoder_AE = keras.Model(image_input, latent_output)\n", "encoder_AE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### The decoder\n", "\n", "The decoder also contains four layers that are reciprocal to those of the encoders, taking the latent representation as the input:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the decoder\n", "latent_input = keras.Input(shape=(latent_dim,))\n", "x = layers.Dense(64, activation=\"relu\")(latent_input)\n", "x = layers.Dense(1024, activation=\"relu\")(x)\n", "x = layers.Reshape((4, 4, 64), input_shape=(1024,))(x)\n", "x = layers.Conv2DTranspose(64, 4, 4, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2DTranspose(32, 4, 4, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "image_output = layers.Conv2D(3, 3, padding = 'same')(x)\n", "#image_output = layers.Reshape((128, 128, 3))(x)\n", "decoder_AE = keras.Model(latent_input, image_output)\n", "decoder_AE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### The autoencoder\n", "\n", "Joining up the encoder and the decoder, we obtain the AE network:\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the AE\n", "image_input = keras.Input(shape=(64, 64, 3))\n", "latent = encoder_AE(image_input)\n", "image_output = decoder_AE(latent)\n", "ae_model = keras.Model(image_input, image_output)\n", "ae_model.summary()\n", "\n", "# compile the AE\n", "ae_model.compile(optimizer='adam', loss='mse', metrics=['mse'])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Train the autoencoder\n", "\n", "Now we can train our AE with the `Galaxy` dataset:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# train the AE\n", "ae_model.fit(train_images, train_images, epochs=50, batch_size=128, \n", "             validation_data=(test_images, test_images))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us inspect how the images are distributed in the latent space. \n", "\n", "### Encode images\n", "\n", "First, we encode the images by our AE. After that, each image becomes a 2D point (because `latent_dim=2`) in the latent space. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# encode images by AE\n", "train_encodings_AE = encoder_AE.predict(train_images)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Scatter plot\n", "We can plot the points in the latent space and colour them by their true labels:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# scatter plot of encodings in the latent space\n", "def scatter_plot_encodings_latent(encodings, labels):\n", "    plt.figure(dpi=100)\n", "    scat = plt.scatter(encodings[:, 0], encodings[:, 1], c=labels, s=.5, cmap='Paired')\n", "    plt.gca().add_artist(plt.legend(*scat.legend_elements(), \n", "                         title='Image labels', bbox_to_anchor=(1.5, 1.)))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.gca().set_aspect(1)\n", "    plt.show()\n", "    \n", "# scatter plot of encodings by AE\n", "scatter_plot_encodings_latent(train_encodings_AE, train_labels)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["###  Histogram plot\n", "\n", "Also, for each galaxy type, we can plot the density histograms of the encodings along the two latent dimensions -- note that we are using the same feature range ($x$-axis) in all the histograms:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# histogram plot of encodings in the latent space\n", "def hist_plot_encodings_latent(encodings, labels, galaxy_id, dim, ax):\n", "    # extract\n", "    encodings_ = encodings[labels == galaxy_id, dim]\n", "    # histogram\n", "    ax.hist(encodings_, bins=60, density=True, color=['g', 'b'][dim], alpha=.5)\n", "    # mean and std dev\n", "    mean = np.mean(encodings_)\n", "    std = np.std(encodings_)\n", "    ax.axvline(mean, c='r')\n", "    ax.set_xlabel('Galaxy shape: %s , Feature %s\\n~${\\cal N}(\\mu=%.1f, \\sigma=%.1f)$' % \n", "                  (string_labels[galaxy_id], ['X', 'Y'][dim], mean, std), c='k')\n", "    \n", "# histogram plot of encodings by AE\n", "fig, axes = plt.subplots(5, 2, dpi=100, figsize=(15, 12), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for galaxy_id in range(5):\n", "    hist_plot_encodings_latent(train_encodings_AE, train_labels, galaxy_id, 0, \n", "                               axes[galaxy_id,  0])\n", "    hist_plot_encodings_latent(train_encodings_AE, train_labels, galaxy_id, 1, \n", "                               axes[galaxy_id, 1])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Regularity of the latent space\n", "\n", "Both the scatter plot and the histogram plots show that the data distributions in the latent space are rather *irregular*. Some of the digits have very wide distributions (such as Barred Spiral).\n", "\n", "Remember that our goal of training this AE is neither dimensionality reduction nor denoising but to generate new images out of the original dataset. Image generation is done by the decoder, taking the latent representation (`X` and `Y` in the plots) as the input. An irregular latent space makes image generation less controllable and robust. Taking our case for example, two shortcomings are likely to emerge:\n", "\n", "1. **Controllability**: sampling the entire latent space, we will generate much more of the widely distributed digits than the narrowly distributed ones; instead, if we limit the range of the latent space, we will loss some characteristics of the widely distributed ones;\n", "\n", "2. **Robustness**: images that do not resemble any of the digits will be generated by the gaps between the distributions of the digits; such gaps increase with the range of the latent space."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Generate new images\n", "\n", "The following function generates new images by uniformly sampling the latent space within a specified range (`[x0, x1]`, `[y0, y1]`). "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images from the latent space\n", "def generate_images_latent(decoder, x0, x1, dx, y0, y1, dy):\n", "    # uniformly sample the latent space\n", "    nx = 10\n", "    ny = 10\n", "    grid_x = np.linspace(x0, x1, nx)\n", "    grid_y = np.linspace(y1, y0, ny)\n", "    latent = np.array(np.meshgrid(grid_x, grid_y)).reshape(2, nx * ny).T\n", "\n", "    # decode images\n", "    decodings = decoder.predict(latent)\n", "    \n", "    # display a (nx, ny) 2D manifold of digits\n", "    figure = np.zeros((64 * ny, 64 * nx, 3))\n", "    for iy in np.arange(ny):\n", "        for ix in np.arange(nx):\n", "            figure[iy * 64 : (iy + 1) * 64, ix * 64 : (ix + 1) * 64, :] = decodings[iy * nx + ix].reshape(64, 64, 3)\n", "            \n", "    # plot figure\n", "    plt.figure(dpi=100, figsize=(nx / 3, ny / 3))\n", "    plt.xticks(np.arange(64 // 2, nx * 64 + 64 // 2, 64), np.round(grid_x, 1), rotation=90)\n", "    plt.yticks(np.arange(64 // 2, ny * 64 + 64 // 2, 64), np.round(grid_y, 1))\n", "    plt.xlabel('Feature X')\n", "    plt.ylabel('Feature Y')\n", "    plt.imshow(figure)\n", "    plt.grid(False)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let us see how the generated images look like. We choose 1-th percentile and 99-th percentile of each axis as minimum and maximum values in the latent space, which encompass all the galaxy shapes and most of the data points. The shortcoming can be observed:\n", "\n", "Only a very few instances of the narrowly distributed galaxies are generated;\n", "\n", "Feel free to try some other ranges. "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images by AE\n", "x_min = int(np.percentile(train_encodings_AE[:, 0], 1))\n", "x_max = int(np.percentile(train_encodings_AE[:, 0], 99))\n", "y_min = int(np.percentile(train_encodings_AE[:, 1], 1))\n", "y_max = int(np.percentile(train_encodings_AE[:, 1], 99))\n", "\n", "generate_images_latent(decoder_AE, x0=x_min, x1=x_max, dx=1, y0=y_min, y1=y_max, dy=1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Variational Autoencoders\n", "\n", "Overfitting is the essential reason behind an irregular latent space of a naive AE, that is, the neural networks for encoding and decoding try their best to fit the data from end to end without caring about how the latent space is organised with respect to the original data. A VAE can regularise the latent space by imposing additional distributional properties on the latent space.\n", "\n", "The following figure summarises **the two extensions** from an AE to a VAE:\n", "\n", "1. Unlike a naive AE that encodes an input data $x$ as a single point $z$ in the latent space, a VAE encodes it as a normal distribution $\\mathcal{N}(\\mu, \\sigma)$, and the latent representation $z$ is sampled from this distribution and then passed to the decoder;\n", "\n", "2. An AE only minimises the reconstruction error $\\lVert x-x'\\rVert^2$ to fit the data, whereas a VAE minimises the sum of the reconstruction error and the KL divergence (Kullback\u2013Leibler divergence) between the latent distribution $\\mathcal{N}(\\mu, \\sigma)$ and the standard normal distribution $\\mathcal{N}(0, 1)$.\n", "\n", "How does a VAE regularise the latent space? The loss function provides a straightforward answer: in addition to fitting the data by minimising the reconstruction error, it also drags the latent distribution to a standard normal distribution. The final model is a trade-off between the two effects. Also, because each input image is encoded as a Gaussian blob instead of a single point, the gaps in the latent space can be filled by such blurring so that meaningless decodings can be largely avoided.\n", "\n", "![ae-vae.png](https://github.com/stfc-sciml/sciml-workshop-v3/blob/master/course_3.0_with_solutions/markdown_pic/ae-vae.png?raw=1)\n", "\n", "\n", "**\ud83d\udcda Theory for VAEs** \n", "\n", "<details> <summary>Reveal / Hide</summary> \n", "<p>\n", "    \n", "\n", "\n", "\n", "### Derivation of ELBO\n", "\n", "Posterior Probability $p(z|x)$, which can be expressed as:\n", "\n", "\\begin{eqnarray}\n", "p(z|x)&=&\\frac{p(x|z)p(z)}{p(x)}\\nonumber\\\\\n", "&=& \\frac{p(x|z)p(z)}{\\int p(x|z)p(z)}\n", "\\end{eqnarray}\n", "\n", "where $\\int p(x|z)p(z)$, which is the marginal, can be intractable and cannot be computed directly. One way  to compute the overall solution $p(z|x)$ is using Monte Carlo methods (such as sampling). The method used in this notebook (and the underlying VAE paper) is  ***variational inference***. \n", "\n", "The idea is to identify another proxy distribution $q(z|x)$ that reasonably approximates $p(z|x)=p(x|z)p(z)$. i.e. if the KL-divergence between two pdfs, $q(x)$ and $p(z|x)$ is denoted by\n", "\n", "$$\\mathrm{KL}(q(x)||p(z|x))$$\n", "\n", "\n", "it can be minimized by selecting an alternative pdf $q(z|x)$, which is a good proxy for $p(z|x)$. But \n", "\n", "\\begin{eqnarray}\n", "\\mathrm{KL}(q(z|x)||p(z|x)) &=& -\\int q(z|x)\\log\\frac{p(z|x)}{q(z|x)} dz\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{p(x)q(z|x)} dz\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz + \\int_{z} q(z|x)\\log p(x)dz \\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)} +  \\log p(x)\\int_{z} q(z|x)dz\\nonumber\\\\   \n", "                          &=& -\\int q(z|x)\\log\\frac{p(x|z)p(z)}{q(z|x)}dz +  \\log p(x)\\nonumber\\\\\n", "                          &=& -\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) \n", "\\end{eqnarray}\n", "\n", "\n", "Given that $\\mathrm{KL}\\left(q(z|x)||p(z|x)\\right)\\geq 0$, \n", "\n", "\n", "\n", "\n", "\\begin{eqnarray}\n", "-\\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz -\\int q(z|x)\\log{p(x|z)}dz + \\log p(x) &\\geq& 0 \\\\\n", "\\log p(x) &\\geq& \\int q(z|x)\\log\\frac{p(z)}{q(z|x)}dz + \\int q(z|x)\\log{p(x|z)}dz\\\\\n", "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\int q(z|x)\\log p(x|z)dz \\nonumber\\\\\n", "\\log p(x) &\\geq& - \\mathrm{KL}(q(z|x)||p(z)) + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \\nonumber\\\\\n", "\\end{eqnarray}\n", "\n", "This is the *variational lower-bound*, or the evidence of lower bound (ELBO).  This remains as the objective function for the VAE. However, frameworks like TensorFlow or PyTorch need a loss function to be minimized. Maximising the log likelihood of the model evidence $p(x)$ is same as minimizing the $-\\log p(x)$. The first term of the ELBO, namely,  $\\mathrm{KL}(q(z|x)||p(z))$ is the *regularising* term and constrains the posterior distribution. The second term of the ELBO models the reconstruction loss. \n", "\n", "Now, this leaves fair bit of freedom on the choice of the prior $p(z)$. Let's assume:\n", "\n", "\n", "$$\n", "p(z)={\\cal N}(\\mu_p, \\sigma_p^2)\n", "$$\n", "\n", "and \n", "\n", "$$\n", "q(z|x)={\\cal N}(\\mu_q, \\sigma_q^2)\n", "$$\n", "\n", "Thus, \n", "\n", "$$\n", "p(z)=\\frac{1}{\\sqrt{2\\pi\\sigma_p^2}}\\exp\\left(\\frac{(x-\\mu_p)^2}{2\\sigma_p^2}\\right)\n", "$$\n", "\n", "and \n", "\n", "$$\n", "q(z|x)=\\frac{1}{\\sqrt{2\\pi\\sigma_q^2}}\\exp\\left(\\frac{(x-\\mu_q)^2}{2\\sigma_q^2}\\right)\n", "$$\n", "\n", "The direct derivation of $\\mathrm{KL}(q(z|x)||p(z))$ will give (with some simplifications)\n", "\n", "\n", "$$\n", "-\\mathrm{KL}(q(z|x)||p(z)) = \\log\\frac{\\sigma_q}{\\sigma_p} - \\frac{\\left(\\log\\sigma_q^2-(\\mu_p-\\mu_q)^2\\right)}{2\\sigma_p^2} +\\frac{1}{2} \n", "$$\n", "\n", "By fixing the prior distribution $p(z)={\\cal N}(0,1^2)$, \n", "\n", "$$\n", "-\\mathrm{KL}(q(z|x)||p(z)) = \\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right]\n", "$$\n", "\n", "Hence, the new ELBO is\n", "\n", "\n", "$$\n", "\\frac{1}{2}\\left[ 1 + \\log\\sigma_q^2 - \\sigma_q^2 -\\mu_q^2\\right] + \\mathbb{E}_{q(z|x)}\\left[\\log p(x|z)\\right] \n", "$$\n", "\n", "\n", "Let $J, B$ and $\\cal{L}$ be the dimension of the latent space, and the batch size over which the sampling is done. The loss function we need to minimise (from the point of  implementation) is\n", "\n", "$$\n", "{\\cal L} = - \\sum_{j=1}^J \\frac{1}{2}\\Bigl[ 1 + \\log\\sigma_j^2 - \\sigma_j^2 -\\mu_j^2\\Bigr] - \\frac{1}{B}\\sum_{l}\\mathbb{E}_{q(z|x_i)}\\left[\\log p(x_i|z^{(i,l)})\\right] \n", "$$\n", "\n", "\n", "\n", "This can be observed in the code implementation below (see function implementation ``loss_function`` below)\n", "\n", "### Reparameterisation\n", "\n", "A valid reparameterization would be \n", "\n", "$$\n", "z = \\mu+\\sigma\\epsilon\n", "$$\n", "\n", "\n", "where $\\epsilon$ is an auxiliary noise variable $\\epsilon\\sim{\\cal{N}}(0, 1)$, which actually enables the reparameterization technique. Although it is possible to use $\\sigma$ or more specifically $\\sigma^2$, working on log scales improves the stability. i.e. \n", "\n", "\\begin{eqnarray}\n", "p &=& \\log(\\sigma^2)\\\\\n", "&=& 2  \\log(\\sigma)\n", "\\end{eqnarray}\n", "\n", "To get the log standard deviation, $\\log(\\sigma)$, \n", "\\begin{eqnarray}\n", "\\log(\\sigma) &=&  p/2 \\\\\n", "\\label{eqn:log_sigma}\n", "\\end{eqnarray} \n", "\n", "and hence\n", "\n", "$$\n", "\\sigma = \\exp^{p/2}\n", "$$\n", "\n", "The resulting estimator (or the loss function) becomes (see Page 5 of [Auto-Encoding Variational Bayes Paper](https://arxiv.org/abs/1312.6114)),\n", "\n", "$$\n", "-\\text{KLD} = \\frac{1}{2}\\sum_{j=1}^{J}(1+\\log(\\sigma_j^i)^2 - (\\mu_j^i)^2 -(\\sigma_j^i)^2)\n", "$$\n", "\n", "\n", "It is important to see that the KL divergence can be computed and differentiated without estimation. This is a very remarkable thing (no esimtation!).\n", "\n", "The $\\boldsymbol{\\epsilon}$ must be sampled from a zero-mean, unit-variance Gaussian distribution, and should be of the same size as $\\boldsymbol{\\sigma}$. \n", "\n", "\n", "<br>\n", "\n", "\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Build and train a VAE\n", "\n", "Now we will implement a VAE. \n", "\n", "\n", "### The encoder\n", "\n", "To implement the probabilistic encoder, we first need a custom function to sample the latent distribution, as implemented by the `Sampling` class. Note that here we are using $\\ln\\sigma$ instead of $\\sigma$ in the network; otherwise, the implementation will be complicated as we have to impose positiveness on $\\sigma$.  \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# sampling z with (z_mean, z_log_var)\n", "class Sampling(layers.Layer):\n", "    def call(self, inputs):\n", "        z_mean, z_log_var = inputs\n", "        epsilon = tf.keras.backend.random_normal(shape=tf.shape(z_mean))\n", "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n", "\n", "\n", "\n", "\n", "# build the encoder\n", "image_input = keras.Input(shape=(64, 64, 3))\n", "x = layers.Conv2D(32, 4, 2, padding = 'same')(image_input)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2D(32, 4, 2, padding = 'same')(image_input)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2D(64, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2D(64, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Flatten()(x)\n", "x = layers.Dense(64, activation=\"relu\")(x)\n", "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(x)\n", "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(x)\n", "z_output = Sampling()([z_mean, z_log_var])\n", "encoder_VAE = keras.Model(image_input, [z_mean, z_log_var, z_output])\n", "encoder_VAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### The decoder\n", "\n", "The docoder is the same as that of AE.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the decoder\n", "latent_input = keras.Input(shape=(latent_dim,))\n", "x = layers.Dense(64, activation=\"relu\")(latent_input)\n", "x = layers.Dense(1024, activation=\"relu\")(x)\n", "x = layers.Reshape((4, 4, 64), input_shape=(1024,))(x)\n", "x = layers.Conv2DTranspose(64, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2DTranspose(64, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2DTranspose(32, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "x = layers.Conv2DTranspose(32, 4, 2, padding = 'same')(x)\n", "x = layers.BatchNormalization()(x)\n", "x = layers.Activation('relu')(x)\n", "image_output = layers.Conv2D(3, 3, activation = 'sigmoid', padding = 'same')(x)\n", "decoder_VAE = keras.Model(latent_input, image_output)\n", "decoder_VAE.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### The VAE\n", "\n", "To add the KL divergence to the loss, we create a class `VAE` derived from `keras.Model` and overwrite its `train_step()` method:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# VAE class\n", "class VAE(keras.Model):\n", "    # constructor\n", "    def __init__(self, encoder, decoder, **kwargs):\n", "        super(VAE, self).__init__(**kwargs)\n", "        self.encoder = encoder\n", "        self.decoder = decoder\n", "\n", "    # customise train_step() to implement the loss \n", "    def train_step(self, x):\n", "        if isinstance(x, tuple):\n", "            x = x[0]\n", "        with tf.GradientTape() as tape:\n", "            # encoding\n", "            z_mean, z_log_var, z = self.encoder(x)\n", "            # decoding\n", "            x_prime = self.decoder(z)\n", "            # reconstruction error by binary crossentropy loss\n", "            #reconstruction_loss = tf.reduce_mean(keras.losses.mean_squared_error(x, x_prime))\n", "            reconstruction_loss = tf.reduce_mean(\n", "                tf.reduce_sum(\n", "                    keras.losses.mean_squared_error(x, x_prime), axis=(1, 2)\n", "                )\n", "            )\n", "            # KL divergence\n", "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n", "            # loss = reconstruction error + KL divergence\n", "            loss = reconstruction_loss + kl_loss\n", "        # apply gradient\n", "        grads = tape.gradient(loss, self.trainable_weights)\n", "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n", "        # return loss for metrics log\n", "        return {\"loss\": loss,\n", "                \"reconstruction_loss\": reconstruction_loss,\n", "                \"kl_loss\": kl_loss}"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can build, compile and train our VAE:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# build the VAE\n", "vae_model = VAE(encoder_VAE, decoder_VAE)\n", "\n", "# compile the VAE\n", "vae_model.compile(optimizer=keras.optimizers.Adam())\n", "\n", "# train the VAE\n", "vae_model.fit(train_images, train_images, epochs=50, batch_size=128)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 5. Analyse results \n", "Next, we can inspect the latent space following the same steps we did for the AE. Clearly, the latent distributions become much more regular than before."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# encode images by VAE\n", "train_encodings_VAE = encoder_VAE.predict(train_images)\n", "\n", "# scatter plot of encodings by VAE\n", "scatter_plot_encodings_latent(train_encodings_VAE[2], train_labels)\n", "\n", "# histogram plot of encodings by VAE\n", "fig, axes = plt.subplots(5, 2, dpi=100, figsize=(15, 12), sharex=True)\n", "plt.subplots_adjust(hspace=.4)\n", "for galaxy_id in range(5):\n", "    hist_plot_encodings_latent(train_encodings_VAE[2], train_labels, galaxy_id, 0, \n", "                               axes[galaxy_id, 0])\n", "    hist_plot_encodings_latent(train_encodings_VAE[2], train_labels, galaxy_id, 1, \n", "                               axes[galaxy_id, 1])\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Generate new images\n", "\n", "Finally, we can generate new images with our VAE. The result shows that, compared to the AE, the numbers of the generated digits have become more in unison and the number of non-digit images has been greatly reduced."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# generate images by AE\n", "x_min = int(np.percentile(train_encodings_VAE[2][:, 0], 2))\n", "x_max = int(np.percentile(train_encodings_VAE[2][:, 0], 98))\n", "y_min = int(np.percentile(train_encodings_VAE[2][:, 1], 2))\n", "y_max = int(np.percentile(train_encodings_VAE[2][:, 1], 98))\n", "\n", "\n", "generate_images_latent(decoder_VAE, x0=x_min, x1=x_max, dx=.1, y0=y_min, y1=y_max, dy=.1)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 6.Exercises\n", "\n", "\n", "* Learn advanced VAEs:\n", "\n", "    * **disentangled VAEs**: how to balance between reconstruction loss (image quality) and variational loss (latent regularity);\n", "    \n", "    * **conditional VAEs**: how to control image generation, e.g., generating an image of a handwritten digit given that digit from 0 to 9."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 4}