{"cells": [{"cell_type": "markdown", "metadata": {"id": "E3Sl_JPQb6PX"}, "source": ["# 0. Three particle types classification\n", "\n", "In this notebook, we attempt to classify three particle types: electron, muon and proton.\n", "\n", "We use a slimmed down dataset of a public dataset from the [Deep Learn Physics Challenge](http://deeplearnphysics.org/DataChallenge/). The dataset is a set of images of three types particles (electron, muon and proton) projected in the xy-plane.\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 6351, "status": "ok", "timestamp": 1646405890610, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "MRxKFB91b6Pa", "outputId": "652ab4d5-a1c6-4336-b551-1e57f7638784"}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization, Activation, LeakyReLU\n", "from tensorflow.keras.callbacks import EarlyStopping\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "# helpers\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "import h5py\n", "from sklearn.model_selection import train_test_split\n", "from os.path import join\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)"]}, {"cell_type": "markdown", "metadata": {"id": "VGA9E6Yyb6Pb"}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 13804, "status": "ok", "timestamp": 1646405904598, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "zyJE5TY2b6Pd"}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 14928, "status": "ok", "timestamp": 1646405919520, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "NIgZs5Rob6Pd", "outputId": "2060ca13-3704-4ea0-f631-0160b5afd7cd"}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {"id": "XbAVWxjyb6Pe"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "6lTvzHlPb6Pf"}, "source": ["# 1. Load the dataset\n", "\n", "### Read raw data\n", "\n", "Our data are stored in the hdf files physics/phy_cls3_train.h5 and physics/phy_cls3_train.h5 containing 15,000 and 3,000 images, respectively. Images are three types particles (electron, muon and proton) projected in the xy-plane. 'phy_cls3_train.h5' includes 5,000 images per particle type and 'phy_cls3_train.h5' includes 1,000 images per particle type. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# load dataset\n", "with h5py.File(join(data_path, 'Physics/phy_cls3_train.h5'), 'r') as F:\n", "\n", "    # downsample images to fit in memory\n", "    x_train = np.array(F['images'][:, ::2, ::2])\n", "    x_train = np.expand_dims(x_train, axis = -1)\n", "    x_train = x_train.astype(np.float32)\n", "    x_train = x_train/255.\n", "    y_train = np.array(F['labels'])\n", "    y_train = y_train.astype(np.float32)\n", "    \n", "\n", "with h5py.File(join(data_path, 'Physics/phy_cls3_test.h5'), 'r') as F:\n", "\n", "    # downsample images to fit in memory\n", "    x_test = np.array(F['images'][:, ::2, ::2])\n", "    x_test = np.expand_dims(x_test, axis = -1)\n", "    x_test = x_test.astype(np.float32)\n", "    x_test =x_test/255.   \n", "    y_test = np.array(F['labels'])\n", "    y_test = y_test.astype(np.float32)\n", "    \n", "\n", "# string labels\n", "string_labels = ['electron', 'muon', 'proton']\n", "\n", "# print info\n", "print(\"Number of training data: %d\" % len(y_train))\n", "print(\"Number of test data: %d\" % len(y_test))\n", "print(\"Image pixels: %s\" % str(x_train[0, :, :, 0].shape))\n", "print(\"Number of channels: %s\" % str(x_train.shape[-1]))\n", "print(\"Number of labels: %s\" % len(string_labels))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 12991, "status": "ok", "timestamp": 1646405947786, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "5YBfEn4xb6Pf", "outputId": "63c0ee48-a103-4d01-8a78-5a97a9c04d64"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 536}, "executionInfo": {"elapsed": 1644, "status": "ok", "timestamp": 1646405952452, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "LmWrK1_qb6Ph", "outputId": "2acf0406-f03c-4cb1-d8f6-6b2a743eaea8"}, "outputs": [], "source": ["# function to plot an image in a subplot\n", "def subplot_image(image, label, nrows=1, ncols=1, iplot=0, label2='', label2_color='r'):\n", "    plt.subplot(nrows, ncols, iplot + 1)\n", "    plt.imshow(image.squeeze())\n", "    plt.xlabel(label, c='k', fontsize=8)\n", "    plt.title(label2, c=label2_color, fontsize=8, y=-0.33)\n", "    plt.xticks([])\n", "    plt.yticks([])\n", "    \n", "# ramdomly plot some images and their labels\n", "nrows = 3\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(len(y_train), nrows * ncols)):\n", "    label = \"%d: %s\" % (y_train[idata], string_labels[int(y_train[idata])])\n", "    subplot_image(x_train[idata], label, nrows, ncols, iplot)\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "e-38VPPBb6Pi"}, "source": ["# 2. Build the network\n", "\n", "The inputs are 2D images. Therefore, we build a network with 2D convolutional layers.\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# build the network architecture\n", "model = Sequential()\n", "model.add(Conv2D(32, (3, 3), strides = 1, input_shape=(128, 128, 1)))\n", "model.add(BatchNormalization())\n", "model.add(LeakyReLU())\n", "model.add(MaxPool2D((2, 2)))\n", "\n", "model.add(Conv2D(64, (3, 3), strides = 1))\n", "model.add(BatchNormalization())\n", "model.add(LeakyReLU())\n", "model.add(MaxPool2D((2, 2)))\n", "\n", "model.add(Conv2D(128, (3, 3), strides = 1))\n", "model.add(BatchNormalization())\n", "model.add(LeakyReLU())\n", "model.add(MaxPool2D((2, 2)))\n", "\n", "model.add(Conv2D(256, (3, 3), strides = 1))\n", "model.add(BatchNormalization())\n", "model.add(LeakyReLU())\n", "model.add(MaxPool2D((2, 2)))\n", "\n", "model.add(Flatten())\n", "model.add(Dense(3, activation='softmax'))\n", "\n", "# print summary\n", "model.summary()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "executionInfo": {"elapsed": 4371, "status": "ok", "timestamp": 1646405959780, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "GGm5IArrb6Pj", "outputId": "3a733e0b-2129-4235-8b75-695306d10388"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "QkmmEsqpb6Pj"}, "source": ["### Compile and train the model\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# optimizer, loss, metrics\n", "opt = tf.keras.optimizers.RMSprop()\n", "model.compile(optimizer=opt,\n", "              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n", "              metrics=['accuracy'])\n", "\n", "# train the model\n", "\n", "history = model.fit(x_train, y_train, epochs=50, batch_size=64,\n", "                             validation_data=(x_test, y_test))\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"colab": {"base_uri": "https://localhost:8080/", "height": 417}, "executionInfo": {"elapsed": 164031, "status": "error", "timestamp": 1646406125083, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "k8d-RqZzb6Pl", "outputId": "18ec5f6a-68fe-41e5-973d-da908f91e9c7"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "9vs5Q1m9b6Pl"}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot training history\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# plot accuracy\n", "plt.figure(dpi=100, figsize=(12, 4))\n", "plt.subplot(1, 2, 1)\n", "plt.plot(history.history[acc_str], label='Accuracy on training data')\n", "plt.plot(history.history['val_' + acc_str], label='Accuracy on test data')\n", "plt.legend()\n", "plt.title(\"Accuracy\")\n", "\n", "# plot loss\n", "plt.subplot(1, 2, 2)\n", "plt.plot(history.history['loss'], label='Loss on training data')\n", "plt.plot(history.history['val_loss'], label='Loss on test data')\n", "plt.legend()\n", "plt.title(\"Loss\")\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 13, "status": "aborted", "timestamp": 1646406125080, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "x2hxL_MDb6Pl"}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {"id": "_tjhTHSAb6Pm"}, "source": ["### Make predictions"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 14, "status": "aborted", "timestamp": 1646406125082, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "KM9ock9eb6Pm"}, "outputs": [], "source": ["# use test images to predict\n", "pred_lables = model.predict(x_test).argmax(axis=1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {"executionInfo": {"elapsed": 14, "status": "aborted", "timestamp": 1646406125083, "user": {"displayName": "Jaehoon Cha", "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64", "userId": "15142223743084603318"}, "user_tz": 0}, "id": "vXEVpoDcb6Pn"}, "outputs": [], "source": ["# get the indices of wrong predictions\n", "id_wrong = np.where(pred_lables != y_test)[0]\n", "print(\"Number of test data: %d\" % y_test.size)\n", "print(\"Number of wrong predictions: %d\" % id_wrong.size)\n", "\n", "# plot the wrong predictions\n", "nrows = 4\n", "ncols = 8\n", "plt.figure(dpi=100, figsize=(ncols * 2, nrows * 2.2))\n", "for iplot, idata in enumerate(np.random.choice(id_wrong, nrows * ncols)):\n", "    label = \"%d: %s\" % (y_test[idata], string_labels[int(y_test[idata])])\n", "    label2 = \"%d: %s\" % (pred_lables[idata], string_labels[pred_lables[idata]])\n", "    subplot_image(x_test[idata], label, nrows, ncols, iplot, label2, 'r')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {"id": "MIaTH0H4b6Po"}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {"id": "PNbO2hnHb6Po"}, "source": ["# 4. Exercises\n", "\n", "\n", "Notice that the accuracies for the training and the test data diverge as the model trains. This is a classic symptom of [overfitting](https://en.wikipedia.org/wiki/Overfitting), that is, our model corresponds too closely to the training data so that it cannot fit the test data with an equivalent accuracy.\n", "\n", "* Add more convolutional layers\n", "* Regularise the network and re-train"]}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "WyNv_AYdb6Po"}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {"id": "EELNeZwyb6Po"}, "outputs": [], "source": []}], "metadata": {"accelerator": "GPU", "colab": {"collapsed_sections": [], "name": "Physics_Three particle types classification_solution.ipynb", "provenance": []}, "kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 1}