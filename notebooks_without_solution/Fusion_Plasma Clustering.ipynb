{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# 0. Plasma Clustering\u200b\n", "\n", "In this notebook, we attempt to cluster time series data using a convolutional Long short-term memory (LSTM) model.\n", "\n", "Thomson scattering is an important tool for plasma diagnostics in nuclear fusion facilities. Thomson Scattering data mapping the electron temperature and density of the profiles are clubbed into 5 groups using our clustering pipeline. The data initially passes through a convolutional LSTM, before PCA which is followed by Mean Shift and Agglomerative Clustering."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# tensorflow\n", "import tensorflow as tf\n", "from tensorflow import keras\n", "from tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Input\n", "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, Flatten, Reshape\n", "from tensorflow.keras.models import Sequential\n", "from tensorflow.keras.layers import TimeDistributed\n", "from tensorflow.keras import layers\n", "from tensorflow.keras.layers import RepeatVector\n", "\n", "# check version\n", "print('Using TensorFlow v%s' % tf.__version__)\n", "acc_str = 'accuracy' if tf.__version__[:2] == '2.' else 'acc'\n", "\n", "\n", "# helpers\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "import matplotlib\n", "from collections import Counter\n", "from scipy import asarray as ar,exp\n", "from sklearn.model_selection import train_test_split\n", "from sklearn import mixture\n", "from sklearn import cluster\n", "from sklearn.decomposition import PCA\n", "import pandas as pd\n", "import matplotlib.patches as mpatches\n", "from sklearn.cluster import AgglomerativeClustering\n", "from sklearn.cluster import MeanShift\n", "import os\n", "from IPython.display import clear_output\n", "import h5py\n", "from os.path import join\n", "\n", "\n", "# need some certainty in data processing\n", "np.random.seed(1234)\n", "tf.random.set_seed(1234)\n", "\n", "data_path = 'data'"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Google Cloud Storage Boilerplate\n", "\n", "The following two cells have some boilerplate to mount the Google Cloud Storage bucket containing the data used for this notebook to your Google Colab file system. **Even you are not using Google Colab, please make sure you run these two cells.** \n", "\n", "To access the data from Google Colab, you need to:\n", "\n", "1. Run the first cell;\n", "2. Follow the link when prompted (you may be asked to log in with your Google account);\n", "3. Copy the Google SDK token back into the prompt and press `Enter`;\n", "4. Run the second cell and wait until the data folder appears.\n", "\n", "If everything works correctly, a new folder called `sciml-workshop-data` should appear in the file browser on the left. Depending on the network speed, this may take one or two minutes. Ignore the warning \"You do not appear to have access to project ...\". If you are running the notebook locally or you have already connected to the bucket, these cells will have no side effects."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# variables passed to bash; do not change\n", "project_id = 'sciml-workshop'\n", "bucket_name = 'sciml-workshop'\n", "colab_data_path = '/content/sciml-workshop-data/'\n", "\n", "try:\n", "    from google.colab import auth\n", "    auth.authenticate_user()\n", "    google_colab_env = 'true'\n", "    data_path = colab_data_path\n", "except:\n", "    google_colab_env = 'false'\n", "    ###################################################\n", "    ######## specify your local data path here ########\n", "    ###################################################\n", "    with open('../local_data_path.txt', 'r') as f: data_path = f.read().splitlines()[0]"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%%bash -s {google_colab_env} {colab_data_path} {project_id} {bucket_name}\n", "\n", "# running locally\n", "if ! $1; then\n", "    echo \"Running notebook locally.\"\n", "    exit\n", "fi\n", "\n", "# already mounted\n", "if [ -d $2 ]; then\n", "    echo \"Data already mounted.\"\n", "    exit\n", "fi\n", "\n", "# mount the bucket\n", "echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n", "curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n", "apt -qq update\n", "apt -qq install gcsfuse\n", "gcloud config set project $3\n", "mkdir $2\n", "gcsfuse --implicit-dirs --limit-bytes-per-sec -1 --limit-ops-per-sec -1 $4 $2"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 1. Load the dataset\n", "\n", "### Read in the Thomson scattering data\n", "\n", "Use pandas to read the hdf 'Fusion/fusion_targets.h5'. 'fusion_targets.h5' includes electron density and electron temperature. All time series follow the same times and shot number. \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "with h5py.File(join(data_path, 'Fusion/fusion_targets.h5'), 'r') as F:\n", "\n", "    shot_num = {}\n", "    for i in range(len(np.array(F['shot_num']))):\n", "        shot_num[i] = int(np.array(F['shot_num'][str(i)]))\n", "        \n", "    time_data = {}\n", "    for i in range(len(np.array(F['time_data']))):\n", "        time_data[i] = np.array(F['time_data'][str(i)])\n", "    \n", "    density_data = {}\n", "    for i in range(len(np.array(F['density_data']))):\n", "        density_data[i] = np.array(F['density_data'][str(i)])\n", "    \n", "    temperature_data = {}\n", "    for i in range(len(np.array(F['temperature_data']))):\n", "        temperature_data[i] = np.array(F['temperature_data'][str(i)])\n", "```\n", "    \n", "</p>\n", "</details>\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## how many instances are measurements taken at per shot\n", "shot_length = []\n", "for i in range(len(time_data)):\n", "    shot_length.append(len(time_data[i]))\n", "\n", "max_shot_length = np.max(shot_length)\n", "print(max_shot_length)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["e_temp = temperature_data\n", "e_density = density_data\n", "shot_num = shot_num"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Add rows of zeros to each shot to equalise the dimensions based on the longest shot length"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["for i in range(len(shot_num)):\n", "    extra_rows = np.zeros((max_shot_length-len(e_temp[i]),110))\n", "    e_temp[i] = np.concatenate((e_temp[i],extra_rows),axis=0)\n", "    e_density[i] = np.concatenate((e_density[i],extra_rows),axis=0)\n", "    \n", "clear_output()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Turn list of 2D arrays into 3D array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["e_density = np.stack(list(e_density.values()))\n", "e_temp = np.stack(list(e_temp.values()))\n", "print(np.shape(e_density))\n", "print(np.shape(e_temp))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Plot electron density and temperature profile for each time "]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# density\n", "shot_index = 500\n", "fig = plt.figure(figsize=(7,5))\n", "for ii in range(len(time_data[shot_index])):\n", "    plt.plot(e_density[shot_index][ii]) \n", "    plt.ylim(0, 0.5*1e20)\n", "    plt.title('Electron Density profile for each time')\n", "    plt.xlabel('Radial Measurement')\n", "    plt.ylabel('Electron Density [m-3]')\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# temperature\n", "shot_index = 500\n", "fig = plt.figure(figsize=(7,5))\n", "for ii in range(len(time_data[shot_index])):\n", "    plt.plot(e_temp[shot_index][ii]) \n", "    plt.ylim(0, 2000)\n", "    plt.title('Electron Temperature profile for each time')\n", "    plt.xlabel('Radial Measurement')\n", "    plt.ylabel('Electron Temperature [eV]')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Visualize an example of time vs radius image of electron temperature and density"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# time vs. radius\n", "shot_index = 500\n", "\n", "plt.figure(figsize=(7,5))\n", "plt.imshow(e_temp[shot_index])\n", "plt.title('Electron Temperature')\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Temperature [eV]')\n", "\n", "plt.figure(figsize=(7,5))\n", "plt.imshow(e_density[shot_index])\n", "plt.title('Electron Density')\n", "plt.xlabel('Radial Measurement')\n", "plt.ylabel('Time Measurement')\n", "plt.colorbar(label='Density [m-3]')"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 2. Build the network\n", "\n", "### Dimensionality reduction via autoencoder and PCA"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## prepare temperature and density arrays for Convolutional LSTM Autoencoder\n", "\n", "new_e_temp = np.reshape(e_temp,(len(e_temp),1,151,110,1))\n", "new_e_density = np.reshape(e_density,(len(e_density),1,151,110,1))\n", "\n", "## normalise data\n", "\n", "new_e_temp = new_e_temp/np.max(new_e_temp)\n", "new_e_density = new_e_density/np.max(new_e_density)\n", "\n", "print(new_e_temp.shape)\n", "print(new_e_density.shape)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Combine temp and density into one array"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["new_e_all = np.concatenate([new_e_temp,new_e_density],axis=4)\n", "np.shape(new_e_all)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Set up the Convolutional LSTM Autoencoder for dimensionality reduction\n", "\n", "Convolutional LSTM takes temp and density as an input. Hence the input shape should be 1$\\times$151$\\times$110$\\times$2."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# this is our input placeholder\n", "input_img = Input(shape=(1,151,110,2))\n", "x = layers.ConvLSTM2D(16, (3, 3), activation='relu', padding='same')(input_img)\n", "x = Reshape((151,110,16))(x)\n", "x = Dropout(0.2)(x)\n", "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n", "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n", "x = layers.Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n", "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n", "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n", "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same')(x)\n", "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n", "\n", "x = TimeDistributed(Flatten())(encoded)\n", "x = Reshape((-1, 80))(x)\n", "\n", "x = LSTM(228, activation='relu', return_sequences=True, stateful=False)(x)\n", "x = Dropout(0.2)(x)\n", "x = LSTM(784, activation='relu', return_sequences=True, stateful=False)(x)\n", "x = Dense(7840, activation='relu')(x)\n", "x = Dropout(0.2)(x)\n", "#x = Dense(16610, activation='relu')(x)\n", "#x = Dropout(0.2)(x)\n", "x = Dense(33220, activation='relu')(x)\n", "decoded = Reshape((1,151,110,2))(x)\n", "\n", "autoencoder = keras.Model(input_img, decoded)\n", "\n", "encoder = keras.Model(input_img, encoded)\n", "print(autoencoder.summary())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Compile and train the model\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n", "\n", "# e_all\n", "x_data = new_e_all\n", "\n", "# run the autoencoder\n", "history = autoencoder.fit(x_data, x_data,\n", "                epochs=50,\n", "                batch_size=64,\n", "                shuffle=True,\n", "                validation_split=1/12.) \n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 3. Analyse results "]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Obtain encodings using the encoder \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# encode the data\n", "encoded_imgs = encoder.predict(x_data) \n", "encoded_imgs = encoded_imgs.reshape(len(encoded_imgs),-1)\n", "print(encoded_imgs.shape)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Check training history"]}, {"cell_type": "markdown", "metadata": {}, "source": ["**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "# loss curves\n", "plt.figure(figsize=(9,6))\n", "plt.plot(history.history['loss'],linewidth=2.5)\n", "plt.plot(history.history['val_loss'],linewidth=2.5)\n", "plt.ylabel('Loss value',fontsize=14)\n", "plt.xlabel('Epoch',fontsize=14)\n", "plt.xticks(fontsize=14)\n", "plt.yticks(fontsize=14)\n", "plt.legend(['Training loss', 'Validation loss'], loc='upper right',fontsize=14)\n", "plt.show()\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Reduce the dimensionality of the encoded data to 2 using PCA\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model_encoder = PCA(n_components=2)\n", "encoded_imgs_2d = model_encoder.fit_transform(encoded_imgs)\n", "print(encoded_imgs_2d.shape)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["df_pca = pd.DataFrame({'pca_x':encoded_imgs_2d[:,0], 'pca_y':encoded_imgs_2d[:,1]})\n", "sns.pairplot(x_vars=['pca_x'], y_vars=['pca_y'], data=df_pca, height=8)\n", "sns.set(style=\"whitegrid\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### reduce the dimensionality of the encoded data to 3using PCA\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model_encoder = PCA(n_components=3)\n", "encoded_imgs_3d = model_encoder.fit_transform(encoded_imgs)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["fig = plt.figure(figsize=(15,15))\n", "ax = fig.add_subplot(111, projection='3d')\n", "ax.scatter(encoded_imgs_3d[:,0],encoded_imgs_3d[:,1],encoded_imgs_3d[:,2])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### reduce the dimensionality of the encoded data to 9 using PCA\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "model_encoder_9 = PCA(n_components=9)\n", "encoded_imgs_9d = model_encoder_9.fit_transform(encoded_imgs)\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "markdown", "metadata": {}, "source": ["### Cluster encoded_imgs_9d using AgglomerativeClustering method \n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "ac_clustering_9 = AgglomerativeClustering(n_clusters=5,linkage='ward').fit(encoded_imgs_9d)\n", "\n", "ac_clustering_9 = ac_clustering_9.labels_\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# define a colour palette for clustering plot\n", "\n", "palette=['steelblue','darkorange','forestgreen','firebrick','purple','sienna','palevioletred','grey']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Devide encoded_imag encoded_imgs_3d with respect to their corresponding ac_clustering_9 label\n", "\n", "**Suggested Answer** \n", "\n", "<details> <summary>Show / Hide</summary> \n", "<p>\n", "    \n", "```python\n", "## show the 9D clustering results in 3D space\n", "\n", "encoded_imgs_3d_0 = []\n", "encoded_imgs_3d_1 = []\n", "encoded_imgs_3d_2 = []\n", "encoded_imgs_3d_3 = []\n", "encoded_imgs_3d_4 = []\n", "\n", "for i in range(len(ac_clustering_9)):\n", "    if ac_clustering_9[i] == 0:\n", "        encoded_imgs_3d_0.append(encoded_imgs_3d[i])\n", "    if ac_clustering_9[i] == 1:\n", "        encoded_imgs_3d_1.append(encoded_imgs_3d[i])\n", "    if ac_clustering_9[i] == 2:\n", "        encoded_imgs_3d_2.append(encoded_imgs_3d[i])\n", "    if ac_clustering_9[i] == 3:\n", "        encoded_imgs_3d_3.append(encoded_imgs_3d[i])\n", "    if ac_clustering_9[i] == 4:\n", "        encoded_imgs_3d_4.append(encoded_imgs_3d[i])\n", "```\n", "    \n", "</p>\n", "</details>"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## 3D representation of encoded data, colour coded by clustering of 9D data\n", "\n", "fig = plt.figure(figsize=(15,15))\n", "ax = fig.add_subplot(111, projection='3d')\n", "ax.scatter(np.array(encoded_imgs_3d_0)[:,0],np.array(encoded_imgs_3d_0)[:,1],np.array(encoded_imgs_3d_0)[:,2])\n", "ax.scatter(np.array(encoded_imgs_3d_1)[:,0],np.array(encoded_imgs_3d_1)[:,1],np.array(encoded_imgs_3d_1)[:,2])\n", "ax.scatter(np.array(encoded_imgs_3d_2)[:,0],np.array(encoded_imgs_3d_2)[:,1],np.array(encoded_imgs_3d_2)[:,2])\n", "ax.scatter(np.array(encoded_imgs_3d_3)[:,0],np.array(encoded_imgs_3d_3)[:,1],np.array(encoded_imgs_3d_3)[:,2])\n", "ax.scatter(np.array(encoded_imgs_3d_4)[:,0],np.array(encoded_imgs_3d_4)[:,1],np.array(encoded_imgs_3d_4)[:,2])\n", "\n", "blue = mpatches.Patch(color=palette[0], label='Cluster 0')\n", "orange = mpatches.Patch(color=palette[1], label='Cluster 1')\n", "green = mpatches.Patch(color=palette[2], label='Cluster 2')\n", "red = mpatches.Patch(color=palette[3], label='Cluster 3')\n", "purple = mpatches.Patch(color=palette[4], label='Cluster 4')\n", "\n", "ax.set_xlabel('PCA 0')\n", "ax.set_ylabel('PCA 1')\n", "ax.set_zlabel('PCA 2')\n", "plt.legend(handles=[blue,orange,green,red,purple])\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# average temperature distribution for each cluster\n", "\n", "e_temp_0 = []\n", "e_temp_1 = []\n", "e_temp_2 = []\n", "e_temp_3 = []\n", "e_temp_4 = []\n", "\n", "for i in range(len(ac_clustering_9)):\n", "    if ac_clustering_9[i] == 0:\n", "        e_temp_0.append(e_temp[i])\n", "    if ac_clustering_9[i] == 1:\n", "        e_temp_1.append(e_temp[i])\n", "    if ac_clustering_9[i] == 2:\n", "        e_temp_2.append(e_temp[i])\n", "    if ac_clustering_9[i] == 3:\n", "        e_temp_3.append(e_temp[i])\n", "    if ac_clustering_9[i] == 4:\n", "        e_temp_4.append(e_temp[i])\n", "        \n", "e_temp_0_mean = np.mean(e_temp_0[:],axis=0)\n", "e_temp_1_mean = np.mean(e_temp_1[:],axis=0)\n", "e_temp_2_mean = np.mean(e_temp_2[:],axis=0)\n", "e_temp_3_mean = np.mean(e_temp_3[:],axis=0)\n", "e_temp_4_mean = np.mean(e_temp_4[:],axis=0)\n", "e_temp_means = (e_temp_0_mean,e_temp_1_mean,e_temp_2_mean,e_temp_3_mean,e_temp_4_mean)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# average density distribution for each cluster\n", "\n", "e_density_0 = []\n", "e_density_1 = []\n", "e_density_2 = []\n", "e_density_3 = []\n", "e_density_4 = []\n", "\n", "for i in range(len(ac_clustering_9)):\n", "    if ac_clustering_9[i] == 0:\n", "        e_density_0.append(e_density[i])\n", "    if ac_clustering_9[i] == 1:\n", "        e_density_1.append(e_density[i])\n", "    if ac_clustering_9[i] == 2:\n", "        e_density_2.append(e_density[i])\n", "    if ac_clustering_9[i] == 3:\n", "        e_density_3.append(e_density[i])\n", "    if ac_clustering_9[i] == 4:\n", "        e_density_4.append(e_density[i])\n", "        \n", "e_density_0_mean = np.mean(e_density_0[:],axis=0)\n", "e_density_1_mean = np.mean(e_density_1[:],axis=0)\n", "e_density_2_mean = np.mean(e_density_2[:],axis=0)\n", "e_density_3_mean = np.mean(e_density_3[:],axis=0)\n", "e_density_4_mean = np.mean(e_density_4[:],axis=0)\n", "e_density_means = (e_density_0_mean,e_density_1_mean,e_density_2_mean,e_density_3_mean,e_density_4_mean)\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(np.min(e_temp_means[:]), np.max(e_temp_means[:]))\n", "print(np.min(e_density_means[:]), np.max(e_density_means[:]))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## plot average temp and density shot distribution for each cluster\n", "\n", "for i in range(len(e_temp_means)):\n", "    print('Cluster '+str(i)+': '+str(Counter(ac_clustering_9)[i])+' shots')\n", "\n", "    fig = plt.figure(figsize=(10,7))\n", "    fig.add_subplot(1,2,1)\n", "    plt.imshow((e_temp_means[i]), vmin=0, vmax=(np.max(e_temp_means[:])))\n", "    plt.title('Electron Temperature')\n", "    plt.xlabel('Radial Measurement')\n", "    plt.ylabel('Time Measurement')\n", "    plt.colorbar(label='Temperature [eV]')\n", "    \n", "    fig.add_subplot(1,2,2)\n", "    plt.imshow(e_density_means[i], vmin=0, vmax=np.max(e_density_means[:]))\n", "    plt.title('Electron Density')\n", "    plt.xlabel('Radial Measurement')\n", "    plt.ylabel('Time Measurement')\n", "    plt.colorbar(label='Density [m-3]')\n", "    \n", "    plt.subplots_adjust(wspace=0.6)\n", "    plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Analysis of time-series clusters"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## reshape temp and density for scatter graphs\n", "\n", "new_e_temp = e_temp.reshape(len(e_temp),-1)\n", "new_e_density = e_density.reshape(len(e_density),-1)\n", "np.shape(np.mean(new_e_temp,axis=1))\n", "\n", "df_time_series = pd.DataFrame({'clusters':ac_clustering_9})"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["## time-series clustering scatter graphs\n", "\n", "## mean temp vs mean density\n", "plt.figure(figsize=(15,10))\n", "ax = sns.scatterplot(x=np.mean(new_e_temp,axis=1), y=np.mean(new_e_density,axis=1), data=df_time_series, hue='clusters',palette=sns.color_palette('coolwarm', as_cmap = True),legend=False)\n", "ax.set(xlabel='Temperature [eV]',ylabel='Density [m-3]')\n", "plt.legend(handles=[blue,orange,green,red,purple],loc='upper left')\n", "plt.show()\n", "\n", "## mean temp\n", "plt.figure(figsize=(15,10))\n", "ax = sns.scatterplot(x=df_time_series.index, y=np.mean(new_e_temp,axis=1), data=df_time_series, hue='clusters',palette=sns.color_palette('coolwarm', as_cmap = True),legend=False)\n", "ax.set(xlabel='Shot Index',ylabel='Temperature [eV]')\n", "plt.legend(handles=[blue,orange,green,red,purple],loc='upper left')\n", "plt.show()\n", "\n", "## mean density\n", "plt.figure(figsize=(15,10))\n", "ax = sns.scatterplot(x=df_time_series.index, y=np.mean(new_e_density,axis=1), data=df_time_series, hue='clusters',palette=sns.color_palette('coolwarm', as_cmap = True),legend=False)\n", "ax.set(xlabel='Shot Index',ylabel='Density [m-3]')\n", "plt.legend(handles=[blue,orange,green,red,purple],loc='upper left')\n", "plt.show()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["---"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# 4. Exercise\n", "\n", "### Dynamic Time Warping (DTW) time-series Clustering\n", "\n", "* Cluster time series data using clustering methods in scikit learn with DTWDistance given below:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def DTWDistance(s1, s2):\n", "    DTW={}\n", "\n", "    for i in range(len(s1)):\n", "        DTW[(i, -1)] = float('inf')\n", "    for i in range(len(s2)):\n", "        DTW[(-1, i)] = float('inf')\n", "    DTW[(-1, -1)] = 0\n", "\n", "    for i in range(len(s1)):\n", "        for j in range(len(s2)):\n", "            dist= (s1[i]-s2[j])**2\n", "            DTW[(i, j)] = dist + min(DTW[(i-1, j)],DTW[(i, j-1)], DTW[(i-1, j-1)])\n", "\n", "    return np.sqrt(DTW[len(s1)-1, len(s2)-1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": []}], "metadata": {"kernelspec": {"display_name": "Python 3 (ipykernel)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.12"}}, "nbformat": 4, "nbformat_minor": 2}